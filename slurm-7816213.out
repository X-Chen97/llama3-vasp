current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
12
Thu Jul 18 16:04:04 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:06:00.0 Off |                    0 |
|  0%   37C    P0              74W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   42C    P0              77W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   36C    P0              73W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   40C    P0              81W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:44<02:14, 44.90s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:15, 45.32s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:16, 45.33s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:16, 45.34s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:24<01:23, 41.95s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.20s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.28s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.21s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:03<00:40, 40.20s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:03<00:40, 40.38s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:03<00:40, 40.44s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:03<00:40, 40.43s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 27.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 32.95s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 27.72s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 32.97s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 27.77s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 32.97s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 27.75s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:11<00:00, 32.97s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama3-vasp/wandb/run-20240718_160738-1zfsj3t2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-4gpu
wandb: â­ï¸ View project at https://wandb.ai/cx9/llama-dft
wandb: ðŸš€ View run at https://wandb.ai/cx9/llama-dft/runs/1zfsj3t2
  0%|          | 0/95 [00:00<?, ?it/s]  1%|          | 1/95 [01:02<1:37:54, 62.49s/it]                                                {'loss': 0.5188, 'grad_norm': 2.04750919342041, 'learning_rate': 2e-05, 'epoch': 0.05}
  1%|          | 1/95 [01:02<1:37:54, 62.49s/it]  2%|â–         | 2/95 [01:49<1:22:37, 53.30s/it]                                                {'loss': 0.7636, 'grad_norm': 2.30938458442688, 'learning_rate': 4e-05, 'epoch': 0.1}
  2%|â–         | 2/95 [01:49<1:22:37, 53.30s/it]  3%|â–Ž         | 3/95 [02:20<1:06:07, 43.13s/it]                                                {'loss': 0.7391, 'grad_norm': 2.0632810592651367, 'learning_rate': 6e-05, 'epoch': 0.15}
  3%|â–Ž         | 3/95 [02:20<1:06:07, 43.13s/it]  4%|â–         | 4/95 [02:42<52:59, 34.94s/it]                                                {'loss': 0.7506, 'grad_norm': 2.2603957653045654, 'learning_rate': 8e-05, 'epoch': 0.21}
  4%|â–         | 4/95 [02:42<52:59, 34.94s/it]  5%|â–Œ         | 5/95 [03:09<47:44, 31.83s/it]                                              {'loss': 0.7747, 'grad_norm': 4.2550859451293945, 'learning_rate': 0.0001, 'epoch': 0.26}
  5%|â–Œ         | 5/95 [03:09<47:44, 31.83s/it]  6%|â–‹         | 6/95 [04:04<59:17, 39.97s/it]                                              {'loss': 0.4729, 'grad_norm': 1.1756892204284668, 'learning_rate': 0.00012, 'epoch': 0.31}
  6%|â–‹         | 6/95 [04:04<59:17, 39.97s/it]  7%|â–‹         | 7/95 [04:40<56:27, 38.50s/it]                                              {'loss': 0.543, 'grad_norm': 3.4329354763031006, 'learning_rate': 0.00014, 'epoch': 0.36}
  7%|â–‹         | 7/95 [04:40<56:27, 38.50s/it]  8%|â–Š         | 8/95 [05:06<50:15, 34.66s/it]                                              {'loss': 0.5511, 'grad_norm': 1.8112967014312744, 'learning_rate': 0.00016, 'epoch': 0.41}
  8%|â–Š         | 8/95 [05:06<50:15, 34.66s/it]  9%|â–‰         | 9/95 [05:25<42:30, 29.65s/it]                                              {'loss': 0.5558, 'grad_norm': 1.1311063766479492, 'learning_rate': 0.00018, 'epoch': 0.46}
  9%|â–‰         | 9/95 [05:25<42:30, 29.65s/it] 11%|â–ˆ         | 10/95 [06:01<44:41, 31.54s/it]                                               {'loss': 0.4987, 'grad_norm': 0.902865469455719, 'learning_rate': 0.0002, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:01<44:41, 31.54s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.14s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:22,  1.47s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:27,  1.97s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  2.00s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.17s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:18,  2.08s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:15,  2.24s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.06s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.15s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.475798100233078, 'eval_runtime': 40.3201, 'eval_samples_per_second': 3.398, 'eval_steps_per_second': 0.446, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:41<44:41, 31.54s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 12%|â–ˆâ–        | 11/95 [07:42<1:13:59, 52.85s/it]                                                 {'loss': 0.4622, 'grad_norm': 0.98268061876297, 'learning_rate': 0.0001999317060143023, 'epoch': 0.57}
 12%|â–ˆâ–        | 11/95 [07:42<1:13:59, 52.85s/it] 13%|â–ˆâ–Ž        | 12/95 [08:16<1:05:13, 47.15s/it]                                                 {'loss': 0.4799, 'grad_norm': 0.8748400807380676, 'learning_rate': 0.00019972691733857883, 'epoch': 0.62}
 13%|â–ˆâ–Ž        | 12/95 [08:16<1:05:13, 47.15s/it] 14%|â–ˆâ–Ž        | 13/95 [08:41<55:24, 40.54s/it]                                                 {'loss': 0.4875, 'grad_norm': 0.7082985043525696, 'learning_rate': 0.0001993859136895274, 'epoch': 0.67}
 14%|â–ˆâ–Ž        | 13/95 [08:41<55:24, 40.54s/it] 15%|â–ˆâ–        | 14/95 [08:59<45:16, 33.54s/it]                                               {'loss': 0.4841, 'grad_norm': 0.5470167994499207, 'learning_rate': 0.0001989091608371146, 'epoch': 0.72}
 15%|â–ˆâ–        | 14/95 [08:59<45:16, 33.54s/it] 16%|â–ˆâ–Œ        | 15/95 [09:47<50:35, 37.95s/it]                                               {'loss': 0.4543, 'grad_norm': 0.4934796392917633, 'learning_rate': 0.0001982973099683902, 'epoch': 0.77}
 16%|â–ˆâ–Œ        | 15/95 [09:47<50:35, 37.95s/it] 17%|â–ˆâ–‹        | 16/95 [10:40<55:52, 42.44s/it]                                               {'loss': 0.4271, 'grad_norm': 0.38300442695617676, 'learning_rate': 0.00019755119679804367, 'epoch': 0.83}
 17%|â–ˆâ–‹        | 16/95 [10:40<55:52, 42.44s/it] 18%|â–ˆâ–Š        | 17/95 [11:12<51:14, 39.42s/it]                                               {'loss': 0.4471, 'grad_norm': 0.4570865333080292, 'learning_rate': 0.00019667184042691875, 'epoch': 0.88}
 18%|â–ˆâ–Š        | 17/95 [11:12<51:14, 39.42s/it] 19%|â–ˆâ–‰        | 18/95 [11:35<44:09, 34.41s/it]                                               {'loss': 0.4949, 'grad_norm': 0.415213018655777, 'learning_rate': 0.0001956604419500441, 'epoch': 0.93}
 19%|â–ˆâ–‰        | 18/95 [11:35<44:09, 34.41s/it] 20%|â–ˆâ–ˆ        | 19/95 [11:50<36:19, 28.68s/it]                                               {'loss': 0.5533, 'grad_norm': 0.7806524038314819, 'learning_rate': 0.00019451838281608197, 'epoch': 0.98}
 20%|â–ˆâ–ˆ        | 19/95 [11:50<36:19, 28.68s/it] 21%|â–ˆâ–ˆ        | 20/95 [12:46<46:13, 36.98s/it]                                               {'loss': 0.3179, 'grad_norm': 0.2684280276298523, 'learning_rate': 0.00019324722294043558, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [12:46<46:13, 36.98s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.26s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.33s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.32s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.4070739150047302, 'eval_runtime': 40.1751, 'eval_samples_per_second': 3.41, 'eval_steps_per_second': 0.448, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [13:27<46:13, 36.98s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 21/95 [14:37<1:12:41, 58.94s/it]                                                 {'loss': 0.4069, 'grad_norm': 0.29640305042266846, 'learning_rate': 0.00019184869857459232, 'epoch': 1.08}
 22%|â–ˆâ–ˆâ–       | 21/95 [14:37<1:12:41, 58.94s/it] 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:12<1:03:07, 51.88s/it]                                                 {'loss': 0.4106, 'grad_norm': 0.3101949393749237, 'learning_rate': 0.0001903247199346129, 'epoch': 1.14}
 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:12<1:03:07, 51.88s/it] 24%|â–ˆâ–ˆâ–       | 23/95 [15:37<52:30, 43.76s/it]                                                 {'loss': 0.4532, 'grad_norm': 0.3678884208202362, 'learning_rate': 0.0001886773685920062, 'epoch': 1.19}
 24%|â–ˆâ–ˆâ–       | 23/95 [15:37<52:30, 43.76s/it] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:53<42:05, 35.58s/it]                                               {'loss': 0.452, 'grad_norm': 0.39892420172691345, 'learning_rate': 0.00018690889463055283, 'epoch': 1.24}
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:53<42:05, 35.58s/it] 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:48<48:04, 41.21s/it]                                               {'loss': 0.3437, 'grad_norm': 0.34791141748428345, 'learning_rate': 0.00018502171357296144, 'epoch': 1.29}
 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:48<48:04, 41.21s/it] 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:35<49:22, 42.93s/it]                                               {'loss': 0.4337, 'grad_norm': 0.338068425655365, 'learning_rate': 0.00018301840308155507, 'epoch': 1.34}
 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:35<49:22, 42.93s/it] 28%|â–ˆâ–ˆâ–Š       | 27/95 [18:05<44:19, 39.11s/it]                                               {'loss': 0.4264, 'grad_norm': 0.3447709083557129, 'learning_rate': 0.00018090169943749476, 'epoch': 1.39}
 28%|â–ˆâ–ˆâ–Š       | 27/95 [18:05<44:19, 39.11s/it] 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:27<38:03, 34.09s/it]                                               {'loss': 0.4229, 'grad_norm': 0.2922482192516327, 'learning_rate': 0.00017867449380334834, 'epoch': 1.45}
 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:27<38:03, 34.09s/it] 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:48<33:00, 30.01s/it]                                               {'loss': 0.4904, 'grad_norm': 0.33871349692344666, 'learning_rate': 0.00017633982827411032, 'epoch': 1.5}
 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:48<33:00, 30.01s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:47<42:00, 38.78s/it]                                               {'loss': 0.3147, 'grad_norm': 0.33200037479400635, 'learning_rate': 0.00017390089172206592, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:47<42:00, 38.78s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.32s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.38441047072410583, 'eval_runtime': 40.5909, 'eval_samples_per_second': 3.375, 'eval_steps_per_second': 0.443, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [20:28<42:00, 38.78s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:39<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:14<56:57, 53.40s/it]                                               {'loss': 0.3888, 'grad_norm': 0.3147869110107422, 'learning_rate': 0.00017136101544117525, 'epoch': 1.6}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:14<56:57, 53.40s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:43<48:07, 45.84s/it]                                               {'loss': 0.3864, 'grad_norm': 0.29236823320388794, 'learning_rate': 0.00016872366859692627, 'epoch': 1.65}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:43<48:07, 45.84s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [22:03<39:26, 38.17s/it]                                               {'loss': 0.4059, 'grad_norm': 0.27224913239479065, 'learning_rate': 0.0001659924534878723, 'epoch': 1.7}
 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [22:03<39:26, 38.17s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:34<36:34, 35.97s/it]                                               {'loss': 0.4291, 'grad_norm': 0.2814442217350006, 'learning_rate': 0.0001631711006253251, 'epoch': 1.75}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:34<36:34, 35.97s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:30<42:02, 42.04s/it]                                               {'loss': 0.3191, 'grad_norm': 0.2945215106010437, 'learning_rate': 0.00016026346363792567, 'epoch': 1.81}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:30<42:02, 42.04s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [24:06<39:42, 40.38s/it]                                               {'loss': 0.4041, 'grad_norm': 0.2740037143230438, 'learning_rate': 0.00015727351400805052, 'epoch': 1.86}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [24:06<39:42, 40.38s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:32<34:49, 36.02s/it]                                               {'loss': 0.3924, 'grad_norm': 0.2756543755531311, 'learning_rate': 0.00015420533564724495, 'epoch': 1.91}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:32<34:49, 36.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:50<29:00, 30.53s/it]                                               {'loss': 0.4269, 'grad_norm': 0.3306032121181488, 'learning_rate': 0.0001510631193180907, 'epoch': 1.96}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:50<29:00, 30.53s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:27<30:24, 32.58s/it]                                               {'loss': 0.4235, 'grad_norm': 0.25058722496032715, 'learning_rate': 0.00014785115691012864, 'epoch': 2.01}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:27<30:24, 32.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:26<37:07, 40.51s/it]                                               {'loss': 0.2882, 'grad_norm': 0.20910927653312683, 'learning_rate': 0.00014457383557765386, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:26<37:07, 40.51s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.33s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.36906999349594116, 'eval_runtime': 40.1946, 'eval_samples_per_second': 3.408, 'eval_steps_per_second': 0.448, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [27:07<37:07, 40.51s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:53<48:55, 54.35s/it]                                               {'loss': 0.3626, 'grad_norm': 0.27922043204307556, 'learning_rate': 0.00014123563174739037, 'epoch': 2.12}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:53<48:55, 54.35s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:20<40:38, 46.00s/it]                                               {'loss': 0.3843, 'grad_norm': 0.2513127028942108, 'learning_rate': 0.00013784110500423104, 'epoch': 2.17}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:20<40:38, 46.00s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:39<32:55, 37.99s/it]                                               {'loss': 0.3606, 'grad_norm': 0.25991567969322205, 'learning_rate': 0.00013439489186339282, 'epoch': 2.22}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:39<32:55, 37.99s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:15<31:51, 37.48s/it]                                               {'loss': 0.3588, 'grad_norm': 0.27994436025619507, 'learning_rate': 0.00013090169943749476, 'epoch': 2.27}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:15<31:51, 37.48s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [30:06<34:28, 41.38s/it]                                               {'loss': 0.3523, 'grad_norm': 0.2121383547782898, 'learning_rate': 0.0001273662990072083, 'epoch': 2.32}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [30:06<34:28, 41.38s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:38<31:36, 38.71s/it]                                               {'loss': 0.3668, 'grad_norm': 0.23696625232696533, 'learning_rate': 0.00012379351950426187, 'epoch': 2.37}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:38<31:36, 38.71s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [31:03<27:33, 34.44s/it]                                               {'loss': 0.3876, 'grad_norm': 0.2227575033903122, 'learning_rate': 0.00012018824091570103, 'epoch': 2.43}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [31:03<27:33, 34.44s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:20<22:51, 29.19s/it]                                               {'loss': 0.3858, 'grad_norm': 0.258180171251297, 'learning_rate': 0.000116555387618413, 'epoch': 2.48}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:20<22:51, 29.19s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:07<26:40, 34.80s/it]                                               {'loss': 0.3478, 'grad_norm': 0.22556985914707184, 'learning_rate': 0.00011289992165302035, 'epoch': 2.53}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:07<26:40, 34.80s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:03<30:48, 41.09s/it]                                               {'loss': 0.3388, 'grad_norm': 0.22544169425964355, 'learning_rate': 0.00010922683594633021, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:03<30:48, 41.09s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.33s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.3539925515651703, 'eval_runtime': 40.1847, 'eval_samples_per_second': 3.409, 'eval_steps_per_second': 0.448, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:43<30:48, 41.09s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:26<39:16, 53.56s/it]                                               {'loss': 0.3428, 'grad_norm': 0.21149495244026184, 'learning_rate': 0.000105541147491597, 'epoch': 2.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:26<39:16, 53.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:50<32:04, 44.75s/it]                                               {'loss': 0.3599, 'grad_norm': 0.2183341085910797, 'learning_rate': 0.00010184789049591299, 'epoch': 2.68}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:50<32:04, 44.75s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:05<25:06, 35.88s/it]                                               {'loss': 0.4474, 'grad_norm': 0.3188142776489258, 'learning_rate': 9.815210950408704e-05, 'epoch': 2.74}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:05<25:06, 35.88s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [36:04<29:13, 42.77s/it]                                               {'loss': 0.2573, 'grad_norm': 0.17629672586917877, 'learning_rate': 9.4458852508403e-05, 'epoch': 2.79}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [36:04<29:13, 42.77s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:44<27:50, 41.77s/it]                                               {'loss': 0.3524, 'grad_norm': 0.2550976574420929, 'learning_rate': 9.077316405366981e-05, 'epoch': 2.84}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:44<27:50, 41.77s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:12<24:35, 37.85s/it]                                               {'loss': 0.3354, 'grad_norm': 0.21018511056900024, 'learning_rate': 8.710007834697969e-05, 'epoch': 2.89}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:12<24:35, 37.85s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:33<20:49, 32.87s/it]                                               {'loss': 0.3766, 'grad_norm': 0.21847157180309296, 'learning_rate': 8.344461238158699e-05, 'epoch': 2.94}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:33<20:49, 32.87s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [37:56<18:23, 29.82s/it]                                               {'loss': 0.4365, 'grad_norm': 0.2585383951663971, 'learning_rate': 7.9811759084299e-05, 'epoch': 2.99}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [37:56<18:23, 29.82s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [38:57<23:24, 39.02s/it]                                               {'loss': 0.2616, 'grad_norm': 0.16122888028621674, 'learning_rate': 7.620648049573815e-05, 'epoch': 3.05}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [38:57<23:24, 39.02s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:39<23:24, 40.14s/it]                                               {'loss': 0.3422, 'grad_norm': 0.244243323802948, 'learning_rate': 7.263370099279172e-05, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:39<23:24, 40.14s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.32s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.34712478518486023, 'eval_runtime': 40.1874, 'eval_samples_per_second': 3.409, 'eval_steps_per_second': 0.448, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [40:20<23:24, 40.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [40:56<28:52, 50.96s/it]                                               {'loss': 0.3201, 'grad_norm': 0.22136321663856506, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.15}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [40:56<28:52, 50.96s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:18<23:17, 42.36s/it]                                               {'loss': 0.3445, 'grad_norm': 0.22357477247714996, 'learning_rate': 6.560510813660719e-05, 'epoch': 3.2}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:18<23:17, 42.36s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:38<19:05, 35.80s/it]                                               {'loss': 0.3595, 'grad_norm': 0.27744966745376587, 'learning_rate': 6.215889499576898e-05, 'epoch': 3.25}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:38<19:05, 35.80s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:39<22:18, 43.19s/it]                                               {'loss': 0.2346, 'grad_norm': 0.1837136298418045, 'learning_rate': 5.876436825260967e-05, 'epoch': 3.3}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:39<22:18, 43.19s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:21<21:22, 42.74s/it]                                               {'loss': 0.3543, 'grad_norm': 0.21736644208431244, 'learning_rate': 5.542616442234618e-05, 'epoch': 3.35}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:21<21:22, 42.74s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:48<18:28, 38.22s/it]                                               {'loss': 0.3298, 'grad_norm': 0.20258986949920654, 'learning_rate': 5.214884308987136e-05, 'epoch': 3.41}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:48<18:28, 38.22s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:09<15:22, 32.95s/it]                                               {'loss': 0.3605, 'grad_norm': 0.21156887710094452, 'learning_rate': 4.893688068190932e-05, 'epoch': 3.46}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:09<15:22, 32.95s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:40<14:33, 32.34s/it]                                               {'loss': 0.3672, 'grad_norm': 0.2887544333934784, 'learning_rate': 4.5794664352755055e-05, 'epoch': 3.51}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:40<14:33, 32.34s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:35<17:02, 39.32s/it]                                               {'loss': 0.2798, 'grad_norm': 0.15966010093688965, 'learning_rate': 4.272648599194948e-05, 'epoch': 3.56}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:35<17:02, 39.32s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:12<16:05, 38.62s/it]                                               {'loss': 0.3083, 'grad_norm': 0.18082576990127563, 'learning_rate': 3.973653636207437e-05, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:12<16:05, 38.62s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.3423527181148529, 'eval_runtime': 40.1874, 'eval_samples_per_second': 3.409, 'eval_steps_per_second': 0.448, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:53<16:05, 38.62s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:24<19:27, 48.65s/it]                                               {'loss': 0.3356, 'grad_norm': 0.204861119389534, 'learning_rate': 3.682889937467493e-05, 'epoch': 3.66}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:24<19:27, 48.65s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:42<15:06, 39.42s/it]                                               {'loss': 0.3265, 'grad_norm': 0.25401952862739563, 'learning_rate': 3.400754651212776e-05, 'epoch': 3.72}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:42<15:06, 39.42s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:24<14:43, 40.16s/it]                                               {'loss': 0.2823, 'grad_norm': 0.49764901399612427, 'learning_rate': 3.1276331403073735e-05, 'epoch': 3.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:24<14:43, 40.16s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:19<15:33, 44.46s/it]                                               {'loss': 0.2975, 'grad_norm': 0.15681369602680206, 'learning_rate': 2.8638984558824777e-05, 'epoch': 3.82}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:19<15:33, 44.46s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [49:54<13:54, 41.73s/it]                                               {'loss': 0.3303, 'grad_norm': 0.19026966392993927, 'learning_rate': 2.6099108277934103e-05, 'epoch': 3.87}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [49:54<13:54, 41.73s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:19<11:36, 36.65s/it]                                               {'loss': 0.3465, 'grad_norm': 0.21615301072597504, 'learning_rate': 2.36601717258897e-05, 'epoch': 3.92}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:19<11:36, 36.65s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:35<09:09, 30.53s/it]                                               {'loss': 0.4056, 'grad_norm': 0.2610218822956085, 'learning_rate': 2.132550619665168e-05, 'epoch': 3.97}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:35<09:09, 30.53s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:26<10:20, 36.51s/it]                                               {'loss': 0.2676, 'grad_norm': 0.1741744428873062, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.03}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:26<10:20, 36.51s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:20<11:12, 42.02s/it]                                               {'loss': 0.2566, 'grad_norm': 0.1382867842912674, 'learning_rate': 1.6981596918444953e-05, 'epoch': 4.08}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:20<11:12, 42.02s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [52:54<09:53, 39.57s/it]                                               {'loss': 0.3227, 'grad_norm': 0.16613855957984924, 'learning_rate': 1.4978286427038601e-05, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [52:54<09:53, 39.57s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.3398951292037964, 'eval_runtime': 40.1911, 'eval_samples_per_second': 3.409, 'eval_steps_per_second': 0.448, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [53:34<09:53, 39.57s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:05<11:25, 48.99s/it]                                               {'loss': 0.3313, 'grad_norm': 0.18009793758392334, 'learning_rate': 1.3091105369447165e-05, 'epoch': 4.18}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:05<11:25, 48.99s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:23<08:34, 39.56s/it]                                               {'loss': 0.2857, 'grad_norm': 0.24271398782730103, 'learning_rate': 1.1322631407993811e-05, 'epoch': 4.23}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:23<08:34, 39.56s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:11<08:26, 42.17s/it]                                               {'loss': 0.3019, 'grad_norm': 0.2094259262084961, 'learning_rate': 9.675280065387116e-06, 'epoch': 4.28}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:11<08:26, 42.17s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:02<08:12, 44.78s/it]                                               {'loss': 0.317, 'grad_norm': 0.15044377744197845, 'learning_rate': 8.151301425407699e-06, 'epoch': 4.34}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:02<08:12, 44.78s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:34<06:50, 41.05s/it]                                               {'loss': 0.3104, 'grad_norm': 0.17901164293289185, 'learning_rate': 6.75277705956443e-06, 'epoch': 4.39}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:34<06:50, 41.05s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [56:57<05:20, 35.63s/it]                                               {'loss': 0.3275, 'grad_norm': 0.1956680864095688, 'learning_rate': 5.481617183918053e-06, 'epoch': 4.44}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [56:57<05:20, 35.63s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:12<03:54, 29.26s/it]                                               {'loss': 0.3874, 'grad_norm': 0.3116818368434906, 'learning_rate': 4.339558049955927e-06, 'epoch': 4.49}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:12<03:54, 29.26s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:11<04:27, 38.24s/it]                                               {'loss': 0.2515, 'grad_norm': 0.15594448149204254, 'learning_rate': 3.3281595730812575e-06, 'epoch': 4.54}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:11<04:27, 38.24s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [58:52<03:55, 39.26s/it]                                               {'loss': 0.3193, 'grad_norm': 0.17695993185043335, 'learning_rate': 2.4488032019563402e-06, 'epoch': 4.59}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [58:52<03:55, 39.26s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:22<03:02, 36.48s/it]                                               {'loss': 0.308, 'grad_norm': 0.18393905460834503, 'learning_rate': 1.7026900316098215e-06, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:22<03:02, 36.48s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:21,  1.45s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:06<00:27,  1.96s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:25,  1.99s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:25,  2.16s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:24,  2.27s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:16<00:23,  2.34s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:17<00:18,  2.07s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.40s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:22<00:15,  2.23s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:25<00:13,  2.33s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:26<00:10,  2.05s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:28<00:07,  1.94s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:31<00:06,  2.14s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:33<00:04,  2.22s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:35<00:01,  1.99s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:37<00:00,  2.17s/it][A                                               
                                               [A{'eval_loss': 0.34144699573516846, 'eval_runtime': 40.2023, 'eval_samples_per_second': 3.408, 'eval_steps_per_second': 0.448, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [1:00:03<03:02, 36.48s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.17s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:32<03:05, 46.28s/it]                                                 {'loss': 0.3188, 'grad_norm': 0.21045368909835815, 'learning_rate': 1.0908391628854041e-06, 'epoch': 4.7}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:32<03:05, 46.28s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:00:57<02:00, 40.01s/it]                                                 {'loss': 0.3439, 'grad_norm': 0.2975143790245056, 'learning_rate': 6.140863104726391e-07, 'epoch': 4.75}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:00:57<02:00, 40.01s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:01:56<01:31, 45.79s/it]                                                 {'loss': 0.2574, 'grad_norm': 0.2193257361650467, 'learning_rate': 2.7308266142119785e-07, 'epoch': 4.8}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:01:56<01:31, 45.79s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:40<00:45, 45.09s/it]                                                 {'loss': 0.32, 'grad_norm': 0.1769990772008896, 'learning_rate': 6.829398569770939e-08, 'epoch': 4.85}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:40<00:45, 45.09s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:07<00:00, 39.70s/it]                                                 {'loss': 0.2942, 'grad_norm': 0.19592070579528809, 'learning_rate': 0.0, 'epoch': 4.9}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:07<00:00, 39.70s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Traceback (most recent call last):
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank1]: Traceback (most recent call last):
[rank1]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank1]:     output = super().train(*args, **kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank1]:     self._load_best_model()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank1]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank1]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank1]:     adapters_weights = safe_load_file(filename, device=device)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank1]:     result[k] = f.get_tensor(k)
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank3]: Traceback (most recent call last):
[rank3]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
[rank3]:     trainer.train()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank3]:     output = super().train(*args, **kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank3]:     self._load_best_model()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank3]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank3]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank3]:     adapters_weights = safe_load_file(filename, device=device)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank3]:     result[k] = f.get_tensor(k)
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank2]: Traceback (most recent call last):
[rank2]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 138, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank2]:     output = super().train(*args, **kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank2]:     self._load_best_model()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank2]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank2]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank2]:     adapters_weights = safe_load_file(filename, device=device)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank2]:     result[k] = f.get_tensor(k)
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
W0718 17:11:17.352000 22624830198272 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 531940 closing signal SIGTERM
W0718 17:11:17.352000 22624830198272 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 531941 closing signal SIGTERM
W0718 17:11:17.352000 22624830198272 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 531943 closing signal SIGTERM
E0718 17:11:18.067000 22624830198272 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 2 (pid: 531942) of binary: /global/scratch/users/chenxin0210/conda-env/llm/bin/python
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/conda-env/llm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run-llama.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-18_17:11:17
  host      : n0048.es1
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 531942)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
