current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
6
Wed Jul 17 01:09:02 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:06:00.0 Off |                    0 |
|  0%   38C    P0              75W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   43C    P0              78W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   34C    P0              73W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   40C    P0              81W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:04,  1.52s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:07<00:08,  4.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:14<00:05,  5.32s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.39s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.67s/it]
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama/wandb/run-20240717_010938-dc10zev3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-5epoch
wandb: ⭐️ View project at https://wandb.ai/cx9/llama-dft
wandb: 🚀 View run at https://wandb.ai/cx9/llama-dft/runs/dc10zev3
  0%|          | 0/95 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  1%|          | 1/95 [03:17<5:09:10, 197.35s/it]                                                   1%|          | 1/95 [03:17<5:09:10, 197.35s/it]  2%|▏         | 2/95 [06:35<5:06:29, 197.73s/it]                                                   2%|▏         | 2/95 [06:35<5:06:29, 197.73s/it]  3%|▎         | 3/95 [09:53<5:03:30, 197.94s/it]                                                   3%|▎         | 3/95 [09:53<5:03:30, 197.94s/it]  4%|▍         | 4/95 [13:03<4:55:29, 194.83s/it]                                                   4%|▍         | 4/95 [13:03<4:55:29, 194.83s/it]  5%|▌         | 5/95 [16:21<4:54:06, 196.07s/it]                                                   5%|▌         | 5/95 [16:21<4:54:06, 196.07s/it]  6%|▋         | 6/95 [19:40<4:51:56, 196.81s/it]                                                   6%|▋         | 6/95 [19:40<4:51:56, 196.81s/it]  7%|▋         | 7/95 [22:58<4:49:20, 197.27s/it]                                                   7%|▋         | 7/95 [22:58<4:49:20, 197.27s/it]  8%|▊         | 8/95 [26:03<4:40:36, 193.52s/it]                                                   8%|▊         | 8/95 [26:03<4:40:36, 193.52s/it]  9%|▉         | 9/95 [29:22<4:39:28, 194.98s/it]                                                   9%|▉         | 9/95 [29:22<4:39:28, 194.98s/it] 11%|█         | 10/95 [32:40<4:37:37, 195.97s/it]                                                   11%|█         | 10/95 [32:40<4:37:37, 195.97s/it]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
{'loss': 0.7364, 'grad_norm': 1.9553760290145874, 'learning_rate': 2e-05, 'epoch': 0.05}
{'loss': 0.7209, 'grad_norm': 1.7699611186981201, 'learning_rate': 4e-05, 'epoch': 0.1}
{'loss': 0.6563, 'grad_norm': 1.5264263153076172, 'learning_rate': 6e-05, 'epoch': 0.15}
{'loss': 0.7864, 'grad_norm': 0.585949718952179, 'learning_rate': 8e-05, 'epoch': 0.21}
{'loss': 0.5849, 'grad_norm': 0.4999660551548004, 'learning_rate': 0.0001, 'epoch': 0.26}
{'loss': 0.5327, 'grad_norm': 0.4414493143558502, 'learning_rate': 0.00012, 'epoch': 0.31}
{'loss': 0.4869, 'grad_norm': 0.40397754311561584, 'learning_rate': 0.00014, 'epoch': 0.36}
{'loss': 0.6329, 'grad_norm': 0.5265498161315918, 'learning_rate': 0.00016, 'epoch': 0.41}
{'loss': 0.4363, 'grad_norm': 0.4845025837421417, 'learning_rate': 0.00018, 'epoch': 0.46}
{'loss': 0.444, 'grad_norm': 0.5360921025276184, 'learning_rate': 0.0002, 'epoch': 0.51}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.03s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.27s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.44s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.47s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A                                                  
                                               [A 11%|█         | 10/95 [35:05<4:37:37, 195.97s/it]
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 12%|█▏        | 11/95 [38:28<5:39:38, 242.61s/it]                                                   12%|█▏        | 11/95 [38:28<5:39:38, 242.61s/it] 13%|█▎        | 12/95 [41:36<5:12:32, 225.94s/it]                                                   13%|█▎        | 12/95 [41:36<5:12:32, 225.94s/it] 14%|█▎        | 13/95 [44:54<4:57:19, 217.55s/it]                                                   14%|█▎        | 13/95 [44:54<4:57:19, 217.55s/it] 15%|█▍        | 14/95 [48:12<4:45:50, 211.73s/it]                                                   15%|█▍        | 14/95 [48:12<4:45:50, 211.73s/it] 16%|█▌        | 15/95 [51:31<4:36:54, 207.68s/it]                                                   16%|█▌        | 15/95 [51:31<4:36:54, 207.68s/it] 17%|█▋        | 16/95 [54:39<4:25:44, 201.83s/it]                                                   17%|█▋        | 16/95 [54:39<4:25:44, 201.83s/it] 18%|█▊        | 17/95 [57:57<4:20:57, 200.74s/it]                                                   18%|█▊        | 17/95 [57:57<4:20:57, 200.74s/it] 19%|█▉        | 18/95 [1:01:15<4:16:35, 199.95s/it]                                                     19%|█▉        | 18/95 [1:01:15<4:16:35, 199.95s/it] 20%|██        | 19/95 [1:04:33<4:12:36, 199.42s/it]                                                     20%|██        | 19/95 [1:04:33<4:12:36, 199.42s/it] 21%|██        | 20/95 [1:06:58<3:48:40, 182.94s/it]                                                     21%|██        | 20/95 [1:06:58<3:48:40, 182.94s/it]{'eval_loss': 0.4029952585697174, 'eval_runtime': 145.541, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 0.51}
{'loss': 0.3566, 'grad_norm': 0.5317628383636475, 'learning_rate': 0.0001999317060143023, 'epoch': 0.56}
{'loss': 0.5422, 'grad_norm': 0.3832859992980957, 'learning_rate': 0.00019972691733857883, 'epoch': 0.62}
{'loss': 0.3859, 'grad_norm': 1.5294476747512817, 'learning_rate': 0.0001993859136895274, 'epoch': 0.67}
{'loss': 0.3292, 'grad_norm': 0.23134243488311768, 'learning_rate': 0.0001989091608371146, 'epoch': 0.72}
{'loss': 0.3456, 'grad_norm': 0.19728504121303558, 'learning_rate': 0.0001982973099683902, 'epoch': 0.77}
{'loss': 0.4967, 'grad_norm': 0.2665468156337738, 'learning_rate': 0.00019755119679804367, 'epoch': 0.82}
{'loss': 0.3552, 'grad_norm': 0.2019340693950653, 'learning_rate': 0.00019667184042691875, 'epoch': 0.87}
{'loss': 0.3284, 'grad_norm': 0.1880408078432083, 'learning_rate': 0.0001956604419500441, 'epoch': 0.92}
{'loss': 0.3535, 'grad_norm': 0.19650770723819733, 'learning_rate': 0.00019451838281608197, 'epoch': 0.97}
{'loss': 0.565, 'grad_norm': 0.45419785380363464, 'learning_rate': 0.00019324722294043558, 'epoch': 1.03}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.35s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.44s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A                                                    
                                               [A 21%|██        | 20/95 [1:09:24<3:48:40, 182.94s/it]
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 22%|██▏       | 21/95 [1:12:47<4:46:55, 232.65s/it]                                                     22%|██▏       | 21/95 [1:12:47<4:46:55, 232.65s/it] 23%|██▎       | 22/95 [1:16:05<4:30:31, 222.34s/it]                                                     23%|██▎       | 22/95 [1:16:05<4:30:31, 222.34s/it] 24%|██▍       | 23/95 [1:19:23<4:18:07, 215.11s/it]                                                     24%|██▍       | 23/95 [1:19:23<4:18:07, 215.11s/it] 25%|██▌       | 24/95 [1:22:31<4:05:04, 207.10s/it]                                                     25%|██▌       | 24/95 [1:22:31<4:05:04, 207.10s/it] 26%|██▋       | 25/95 [1:25:50<3:58:32, 204.47s/it]                                                     26%|██▋       | 25/95 [1:25:50<3:58:32, 204.47s/it] 27%|██▋       | 26/95 [1:29:08<3:53:00, 202.62s/it]                                                     27%|██▋       | 26/95 [1:29:08<3:53:00, 202.62s/it] 28%|██▊       | 27/95 [1:32:26<3:48:10, 201.33s/it]                                                     28%|██▊       | 27/95 [1:32:26<3:48:10, 201.33s/it] 29%|██▉       | 28/95 [1:35:36<3:40:52, 197.80s/it]                                                     29%|██▉       | 28/95 [1:35:36<3:40:52, 197.80s/it] 31%|███       | 29/95 [1:38:54<3:37:43, 197.93s/it]                                                     31%|███       | 29/95 [1:38:54<3:37:43, 197.93s/it] 32%|███▏      | 30/95 [1:42:13<3:34:33, 198.05s/it]                                                     32%|███▏      | 30/95 [1:42:13<3:34:33, 198.05s/it]{'eval_loss': 0.33288049697875977, 'eval_runtime': 145.5909, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 1.03}
{'loss': 0.3395, 'grad_norm': 0.16815485060214996, 'learning_rate': 0.00019184869857459232, 'epoch': 1.08}
{'loss': 0.3043, 'grad_norm': 0.13357390463352203, 'learning_rate': 0.0001903247199346129, 'epoch': 1.13}
{'loss': 0.2813, 'grad_norm': 0.13252025842666626, 'learning_rate': 0.0001886773685920062, 'epoch': 1.18}
{'loss': 0.3946, 'grad_norm': 0.1532435119152069, 'learning_rate': 0.00018690889463055283, 'epoch': 1.23}
{'loss': 0.3003, 'grad_norm': 0.12608292698860168, 'learning_rate': 0.00018502171357296144, 'epoch': 1.28}
{'loss': 0.3183, 'grad_norm': 0.1411333680152893, 'learning_rate': 0.00018301840308155507, 'epoch': 1.33}
{'loss': 0.3003, 'grad_norm': 0.13113731145858765, 'learning_rate': 0.00018090169943749476, 'epoch': 1.38}
{'loss': 0.4069, 'grad_norm': 0.16223779320716858, 'learning_rate': 0.00017867449380334834, 'epoch': 1.44}
{'loss': 0.3296, 'grad_norm': 0.12783773243427277, 'learning_rate': 0.00017633982827411032, 'epoch': 1.49}
{'loss': 0.3, 'grad_norm': 0.13947460055351257, 'learning_rate': 0.00017390089172206592, 'epoch': 1.54}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.01s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.45s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A                                                    
                                               [A 32%|███▏      | 30/95 [1:44:38<3:34:33, 198.05s/it]
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 33%|███▎      | 31/95 [1:48:01<4:19:23, 243.18s/it]                                                     33%|███▎      | 31/95 [1:48:01<4:19:23, 243.18s/it] 34%|███▎      | 32/95 [1:51:05<3:56:49, 225.55s/it]                                                     34%|███▎      | 32/95 [1:51:05<3:56:49, 225.55s/it] 35%|███▍      | 33/95 [1:54:24<3:44:37, 217.38s/it]                                                     35%|███▍      | 33/95 [1:54:24<3:44:37, 217.38s/it] 36%|███▌      | 34/95 [1:57:42<3:35:12, 211.67s/it]                                                     36%|███▌      | 34/95 [1:57:42<3:35:12, 211.67s/it] 37%|███▋      | 35/95 [2:01:00<3:27:39, 207.66s/it]                                                     37%|███▋      | 35/95 [2:01:00<3:27:39, 207.66s/it] 38%|███▊      | 36/95 [2:04:09<3:18:29, 201.86s/it]                                                     38%|███▊      | 36/95 [2:04:09<3:18:29, 201.86s/it] 39%|███▉      | 37/95 [2:07:27<3:14:07, 200.82s/it]                                                     39%|███▉      | 37/95 [2:07:27<3:14:07, 200.82s/it] 40%|████      | 38/95 [2:10:45<3:10:03, 200.06s/it]                                                     40%|████      | 38/95 [2:10:45<3:10:03, 200.06s/it] 41%|████      | 39/95 [2:13:07<2:50:19, 182.48s/it]                                                     41%|████      | 39/95 [2:13:07<2:50:19, 182.48s/it] 42%|████▏     | 40/95 [2:16:25<2:51:37, 187.23s/it]                                                     42%|████▏     | 40/95 [2:16:25<2:51:37, 187.23s/it]{'eval_loss': 0.31686633825302124, 'eval_runtime': 145.6012, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 1.54}
{'loss': 0.3317, 'grad_norm': 0.11713356524705887, 'learning_rate': 0.00017136101544117525, 'epoch': 1.59}
{'loss': 0.4869, 'grad_norm': 0.15597206354141235, 'learning_rate': 0.00016872366859692627, 'epoch': 1.64}
{'loss': 0.2805, 'grad_norm': 0.11279458552598953, 'learning_rate': 0.0001659924534878723, 'epoch': 1.69}
{'loss': 0.2692, 'grad_norm': 0.11676052212715149, 'learning_rate': 0.0001631711006253251, 'epoch': 1.74}
{'loss': 0.2798, 'grad_norm': 0.10423167049884796, 'learning_rate': 0.00016026346363792567, 'epoch': 1.79}
{'loss': 0.4237, 'grad_norm': 0.1286749243736267, 'learning_rate': 0.00015727351400805052, 'epoch': 1.85}
{'loss': 0.302, 'grad_norm': 0.12205741554498672, 'learning_rate': 0.00015420533564724495, 'epoch': 1.9}
{'loss': 0.3228, 'grad_norm': 0.11527114361524582, 'learning_rate': 0.0001510631193180907, 'epoch': 1.95}
{'loss': 0.5352, 'grad_norm': 0.2718648314476013, 'learning_rate': 0.00014785115691012864, 'epoch': 2.0}
{'loss': 0.3007, 'grad_norm': 0.11958718299865723, 'learning_rate': 0.00014457383557765386, 'epoch': 2.05}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:08,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.44s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A                                                    
                                               [A 42%|████▏     | 40/95 [2:18:51<2:51:37, 187.23s/it]
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 43%|████▎     | 41/95 [2:22:14<3:32:03, 235.62s/it]                                                     43%|████▎     | 41/95 [2:22:14<3:32:03, 235.62s/it] 44%|████▍     | 42/95 [2:25:32<3:18:13, 224.41s/it]                                                     44%|████▍     | 42/95 [2:25:32<3:18:13, 224.41s/it] 45%|████▌     | 43/95 [2:28:40<3:05:02, 213.50s/it]                                                     45%|████▌     | 43/95 [2:28:40<3:05:02, 213.50s/it] 46%|████▋     | 44/95 [2:31:58<2:57:36, 208.96s/it]                                                     46%|████▋     | 44/95 [2:31:58<2:57:36, 208.96s/it] 47%|████▋     | 45/95 [2:35:17<2:51:27, 205.76s/it]                                                     47%|████▋     | 45/95 [2:35:17<2:51:27, 205.76s/it] 48%|████▊     | 46/95 [2:38:35<2:46:12, 203.52s/it]                                                     48%|████▊     | 46/95 [2:38:35<2:46:12, 203.52s/it] 49%|████▉     | 47/95 [2:41:41<2:38:37, 198.28s/it]                                                     49%|████▉     | 47/95 [2:41:41<2:38:37, 198.28s/it] 51%|█████     | 48/95 [2:44:59<2:35:19, 198.28s/it]                                                     51%|█████     | 48/95 [2:44:59<2:35:19, 198.28s/it] 52%|█████▏    | 49/95 [2:48:18<2:32:01, 198.29s/it]                                                     52%|█████▏    | 49/95 [2:48:18<2:32:01, 198.29s/it] 53%|█████▎    | 50/95 [2:51:36<2:28:42, 198.29s/it]                                                     53%|█████▎    | 50/95 [2:51:36<2:28:42, 198.29s/it]{'eval_loss': 0.3084036707878113, 'eval_runtime': 145.5919, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 2.05}
{'loss': 0.2814, 'grad_norm': 0.12490475177764893, 'learning_rate': 0.00014123563174739037, 'epoch': 2.1}
{'loss': 0.2696, 'grad_norm': 0.12322795391082764, 'learning_rate': 0.00013784110500423104, 'epoch': 2.15}
{'loss': 0.3495, 'grad_norm': 0.1545262187719345, 'learning_rate': 0.00013439489186339282, 'epoch': 2.21}
{'loss': 0.2943, 'grad_norm': 0.11797278374433517, 'learning_rate': 0.00013090169943749476, 'epoch': 2.26}
{'loss': 0.2695, 'grad_norm': 0.11551181972026825, 'learning_rate': 0.0001273662990072083, 'epoch': 2.31}
{'loss': 0.3131, 'grad_norm': 0.11577794700860977, 'learning_rate': 0.00012379351950426187, 'epoch': 2.36}
{'loss': 0.4208, 'grad_norm': 0.15078985691070557, 'learning_rate': 0.00012018824091570103, 'epoch': 2.41}
{'loss': 0.3127, 'grad_norm': 0.1293381154537201, 'learning_rate': 0.000116555387618413, 'epoch': 2.46}
{'loss': 0.2766, 'grad_norm': 0.11566097289323807, 'learning_rate': 0.00011289992165302035, 'epoch': 2.51}
{'loss': 0.2791, 'grad_norm': 0.12308469414710999, 'learning_rate': 0.00010922683594633021, 'epoch': 2.56}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.03s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.44s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.47s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A                                                    
                                               [A 53%|█████▎    | 50/95 [2:54:01<2:28:42, 198.29s/it]
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 54%|█████▎    | 51/95 [2:57:12<2:55:43, 239.62s/it]                                                     54%|█████▎    | 51/95 [2:57:12<2:55:43, 239.62s/it] 55%|█████▍    | 52/95 [3:00:30<2:42:49, 227.20s/it]                                                     55%|█████▍    | 52/95 [3:00:30<2:42:49, 227.20s/it] 56%|█████▌    | 53/95 [3:03:48<2:32:57, 218.50s/it]                                                     56%|█████▌    | 53/95 [3:03:48<2:32:57, 218.50s/it] 57%|█████▋    | 54/95 [3:07:07<2:25:08, 212.41s/it]                                                     57%|█████▋    | 54/95 [3:07:07<2:25:08, 212.41s/it] 58%|█████▊    | 55/95 [3:10:16<2:17:01, 205.54s/it]                                                     58%|█████▊    | 55/95 [3:10:16<2:17:01, 205.54s/it] 59%|█████▉    | 56/95 [3:13:34<2:12:09, 203.32s/it]                                                     59%|█████▉    | 56/95 [3:13:34<2:12:09, 203.32s/it] 60%|██████    | 57/95 [3:16:53<2:07:48, 201.81s/it]                                                     60%|██████    | 57/95 [3:16:53<2:07:48, 201.81s/it] 61%|██████    | 58/95 [3:20:11<2:03:46, 200.72s/it]                                                     61%|██████    | 58/95 [3:20:11<2:03:46, 200.72s/it] 62%|██████▏   | 59/95 [3:22:34<1:50:03, 183.44s/it]                                                     62%|██████▏   | 59/95 [3:22:34<1:50:03, 183.44s/it] 63%|██████▎   | 60/95 [3:25:52<1:49:35, 187.89s/it]                                                     63%|██████▎   | 60/95 [3:25:52<1:49:35, 187.89s/it]{'eval_loss': 0.3008694648742676, 'eval_runtime': 145.5685, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 2.56}
{'loss': 0.4051, 'grad_norm': 0.13849689066410065, 'learning_rate': 0.000105541147491597, 'epoch': 2.62}
{'loss': 0.2592, 'grad_norm': 0.11821393668651581, 'learning_rate': 0.00010184789049591299, 'epoch': 2.67}
{'loss': 0.2457, 'grad_norm': 0.12096191197633743, 'learning_rate': 9.815210950408704e-05, 'epoch': 2.72}
{'loss': 0.2673, 'grad_norm': 0.11931265890598297, 'learning_rate': 9.4458852508403e-05, 'epoch': 2.77}
{'loss': 0.4699, 'grad_norm': 0.15501554310321808, 'learning_rate': 9.077316405366981e-05, 'epoch': 2.82}
{'loss': 0.3141, 'grad_norm': 0.12589558959007263, 'learning_rate': 8.710007834697969e-05, 'epoch': 2.87}
{'loss': 0.2651, 'grad_norm': 0.13380324840545654, 'learning_rate': 8.344461238158699e-05, 'epoch': 2.92}
{'loss': 0.2853, 'grad_norm': 0.124652199447155, 'learning_rate': 7.9811759084299e-05, 'epoch': 2.97}
{'loss': 0.431, 'grad_norm': 0.30037474632263184, 'learning_rate': 7.620648049573815e-05, 'epoch': 3.03}
{'loss': 0.2448, 'grad_norm': 0.12711116671562195, 'learning_rate': 7.263370099279172e-05, 'epoch': 3.08}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.95s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.49s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.35s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.45s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A                                                    
                                               [A 63%|██████▎   | 60/95 [3:28:18<1:49:35, 187.89s/it]
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 64%|██████▍   | 61/95 [3:31:41<2:13:46, 236.08s/it]                                                     64%|██████▍   | 61/95 [3:31:41<2:13:46, 236.08s/it] 65%|██████▌   | 62/95 [3:34:59<2:03:36, 224.75s/it]                                                     65%|██████▌   | 62/95 [3:34:59<2:03:36, 224.75s/it] 66%|██████▋   | 63/95 [3:38:08<1:54:10, 214.09s/it]                                                     66%|██████▋   | 63/95 [3:38:08<1:54:10, 214.09s/it] 67%|██████▋   | 64/95 [3:41:26<1:48:09, 209.33s/it]                                                     67%|██████▋   | 64/95 [3:41:26<1:48:09, 209.33s/it] 68%|██████▊   | 65/95 [3:44:45<1:43:00, 206.00s/it]                                                     68%|██████▊   | 65/95 [3:44:45<1:43:00, 206.00s/it] 69%|██████▉   | 66/95 [3:48:03<1:38:26, 203.67s/it]                                                     69%|██████▉   | 66/95 [3:48:03<1:38:26, 203.67s/it] 71%|███████   | 67/95 [3:51:11<1:32:51, 198.97s/it]                                                     71%|███████   | 67/95 [3:51:11<1:32:51, 198.97s/it] 72%|███████▏  | 68/95 [3:54:29<1:29:25, 198.74s/it]                                                     72%|███████▏  | 68/95 [3:54:29<1:29:25, 198.74s/it] 73%|███████▎  | 69/95 [3:57:47<1:26:03, 198.59s/it]                                                     73%|███████▎  | 69/95 [3:57:47<1:26:03, 198.59s/it] 74%|███████▎  | 70/95 [4:01:06<1:22:42, 198.51s/it]                                                     74%|███████▎  | 70/95 [4:01:06<1:22:42, 198.51s/it]{'eval_loss': 0.29637420177459717, 'eval_runtime': 145.6267, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 3.08}
{'loss': 0.2865, 'grad_norm': 0.13579581677913666, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.13}
{'loss': 0.275, 'grad_norm': 0.12182112038135529, 'learning_rate': 6.560510813660719e-05, 'epoch': 3.18}
{'loss': 0.3687, 'grad_norm': 0.1574421226978302, 'learning_rate': 6.215889499576898e-05, 'epoch': 3.23}
{'loss': 0.3016, 'grad_norm': 0.1260324865579605, 'learning_rate': 5.876436825260967e-05, 'epoch': 3.28}
{'loss': 0.2634, 'grad_norm': 0.10799987614154816, 'learning_rate': 5.542616442234618e-05, 'epoch': 3.33}
{'loss': 0.2799, 'grad_norm': 0.129380464553833, 'learning_rate': 5.214884308987136e-05, 'epoch': 3.38}
{'loss': 0.403, 'grad_norm': 0.132938414812088, 'learning_rate': 4.893688068190932e-05, 'epoch': 3.44}
{'loss': 0.2729, 'grad_norm': 0.12818482518196106, 'learning_rate': 4.5794664352755055e-05, 'epoch': 3.49}
{'loss': 0.2651, 'grad_norm': 0.11866465955972672, 'learning_rate': 4.272648599194948e-05, 'epoch': 3.54}
{'loss': 0.2396, 'grad_norm': 0.11272106319665909, 'learning_rate': 3.973653636207437e-05, 'epoch': 3.59}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.01s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.45s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A                                                    
                                               [A 74%|███████▎  | 70/95 [4:03:31<1:22:42, 198.51s/it]
100%|██████████| 18/18 [02:19<00:00,  6.99s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 75%|███████▍  | 71/95 [4:06:38<1:35:30, 238.79s/it]                                                     75%|███████▍  | 71/95 [4:06:38<1:35:30, 238.79s/it] 76%|███████▌  | 72/95 [4:09:57<1:26:52, 226.62s/it]                                                     76%|███████▌  | 72/95 [4:09:57<1:26:52, 226.62s/it] 77%|███████▋  | 73/95 [4:13:15<1:19:58, 218.13s/it]                                                     77%|███████▋  | 73/95 [4:13:15<1:19:58, 218.13s/it] 78%|███████▊  | 74/95 [4:16:33<1:14:15, 212.17s/it]                                                     78%|███████▊  | 74/95 [4:16:33<1:14:15, 212.17s/it] 79%|███████▉  | 75/95 [4:19:43<1:08:27, 205.37s/it]                                                     79%|███████▉  | 75/95 [4:19:43<1:08:27, 205.37s/it] 80%|████████  | 76/95 [4:23:01<1:04:21, 203.23s/it]                                                     80%|████████  | 76/95 [4:23:01<1:04:21, 203.23s/it] 81%|████████  | 77/95 [4:26:19<1:00:31, 201.74s/it]                                                     81%|████████  | 77/95 [4:26:19<1:00:31, 201.74s/it] 82%|████████▏ | 78/95 [4:28:41<52:05, 183.83s/it]                                                     82%|████████▏ | 78/95 [4:28:41<52:05, 183.83s/it] 83%|████████▎ | 79/95 [4:32:00<50:10, 188.18s/it]                                                   83%|████████▎ | 79/95 [4:32:00<50:10, 188.18s/it] 84%|████████▍ | 80/95 [4:35:18<47:48, 191.23s/it]                                                   84%|████████▍ | 80/95 [4:35:18<47:48, 191.23s/it]{'eval_loss': 0.29339271783828735, 'eval_runtime': 145.6093, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 3.59}
{'loss': 0.3838, 'grad_norm': 0.14180509746074677, 'learning_rate': 3.682889937467493e-05, 'epoch': 3.64}
{'loss': 0.293, 'grad_norm': 0.12669065594673157, 'learning_rate': 3.400754651212776e-05, 'epoch': 3.69}
{'loss': 0.2623, 'grad_norm': 0.1302546262741089, 'learning_rate': 3.1276331403073735e-05, 'epoch': 3.74}
{'loss': 0.2986, 'grad_norm': 0.12104550749063492, 'learning_rate': 2.8638984558824777e-05, 'epoch': 3.79}
{'loss': 0.4011, 'grad_norm': 0.1459769457578659, 'learning_rate': 2.6099108277934103e-05, 'epoch': 3.85}
{'loss': 0.2668, 'grad_norm': 0.11837724596261978, 'learning_rate': 2.36601717258897e-05, 'epoch': 3.9}
{'loss': 0.2345, 'grad_norm': 0.1171051487326622, 'learning_rate': 2.132550619665168e-05, 'epoch': 3.95}
{'loss': 0.4321, 'grad_norm': 0.29613006114959717, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.0}
{'loss': 0.254, 'grad_norm': 0.10914422571659088, 'learning_rate': 1.6981596918444953e-05, 'epoch': 4.05}
{'loss': 0.2788, 'grad_norm': 0.11829105019569397, 'learning_rate': 1.4978286427038601e-05, 'epoch': 4.1}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.02s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.04s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.28s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.35s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.45s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A                                                  
                                               [A 84%|████████▍ | 80/95 [4:37:44<47:48, 191.23s/it]
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 85%|████████▌ | 81/95 [4:41:06<55:38, 238.43s/it]                                                   85%|████████▌ | 81/95 [4:41:06<55:38, 238.43s/it] 86%|████████▋ | 82/95 [4:44:15<48:23, 223.33s/it]                                                   86%|████████▋ | 82/95 [4:44:15<48:23, 223.33s/it] 87%|████████▋ | 83/95 [4:47:33<43:09, 215.82s/it]                                                   87%|████████▋ | 83/95 [4:47:33<43:09, 215.82s/it] 88%|████████▊ | 84/95 [4:50:51<38:36, 210.55s/it]                                                   88%|████████▊ | 84/95 [4:50:51<38:36, 210.55s/it] 89%|████████▉ | 85/95 [4:54:09<34:28, 206.88s/it]                                                   89%|████████▉ | 85/95 [4:54:09<34:28, 206.88s/it] 91%|█████████ | 86/95 [4:57:19<30:16, 201.82s/it]                                                   91%|█████████ | 86/95 [4:57:19<30:16, 201.82s/it] 92%|█████████▏| 87/95 [5:00:38<26:46, 200.75s/it]                                                   92%|█████████▏| 87/95 [5:00:38<26:46, 200.75s/it] 93%|█████████▎| 88/95 [5:03:56<23:19, 200.00s/it]                                                   93%|█████████▎| 88/95 [5:03:56<23:19, 200.00s/it] 94%|█████████▎| 89/95 [5:07:14<19:56, 199.48s/it]                                                   94%|█████████▎| 89/95 [5:07:14<19:56, 199.48s/it] 95%|█████████▍| 90/95 [5:10:20<16:16, 195.38s/it]                                                   95%|█████████▍| 90/95 [5:10:20<16:16, 195.38s/it]{'eval_loss': 0.29151737689971924, 'eval_runtime': 145.5946, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 4.1}
{'loss': 0.2548, 'grad_norm': 0.10943768918514252, 'learning_rate': 1.3091105369447165e-05, 'epoch': 4.15}
{'loss': 0.3718, 'grad_norm': 0.1197357326745987, 'learning_rate': 1.1322631407993811e-05, 'epoch': 4.21}
{'loss': 0.2575, 'grad_norm': 0.11066854745149612, 'learning_rate': 9.675280065387116e-06, 'epoch': 4.26}
{'loss': 0.2364, 'grad_norm': 0.10697545856237411, 'learning_rate': 8.151301425407699e-06, 'epoch': 4.31}
{'loss': 0.2491, 'grad_norm': 0.1136893704533577, 'learning_rate': 6.75277705956443e-06, 'epoch': 4.36}
{'loss': 0.399, 'grad_norm': 0.15768775343894958, 'learning_rate': 5.481617183918053e-06, 'epoch': 4.41}
{'loss': 0.2534, 'grad_norm': 0.11320251226425171, 'learning_rate': 4.339558049955927e-06, 'epoch': 4.46}
{'loss': 0.2368, 'grad_norm': 0.10557863861322403, 'learning_rate': 3.3281595730812575e-06, 'epoch': 4.51}
{'loss': 0.307, 'grad_norm': 0.12189632654190063, 'learning_rate': 2.4488032019563402e-06, 'epoch': 4.56}
{'loss': 0.4392, 'grad_norm': 0.14131467044353485, 'learning_rate': 1.7026900316098215e-06, 'epoch': 4.62}

  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:08<01:07,  4.25s/it][A
 17%|█▋        | 3/18 [00:16<01:30,  6.01s/it][A
 22%|██▏       | 4/18 [00:25<01:37,  6.94s/it][A
 28%|██▊       | 5/18 [00:33<01:37,  7.48s/it][A
 33%|███▎      | 6/18 [00:42<01:33,  7.82s/it][A
 39%|███▉      | 7/18 [00:50<01:28,  8.03s/it][A
 44%|████▍     | 8/18 [00:59<01:21,  8.18s/it][A
 50%|█████     | 9/18 [01:07<01:14,  8.27s/it][A
 56%|█████▌    | 10/18 [01:16<01:06,  8.34s/it][A
 61%|██████    | 11/18 [01:24<00:58,  8.39s/it][A
 67%|██████▋   | 12/18 [01:33<00:50,  8.42s/it][A
 72%|███████▏  | 13/18 [01:41<00:42,  8.44s/it][A
 78%|███████▊  | 14/18 [01:50<00:33,  8.46s/it][A
 83%|████████▎ | 15/18 [01:58<00:25,  8.47s/it][A
 89%|████████▉ | 16/18 [02:07<00:16,  8.48s/it][A
 94%|█████████▍| 17/18 [02:15<00:08,  8.48s/it][A
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A                                                  
                                               [A 95%|█████████▍| 90/95 [5:12:46<16:16, 195.38s/it]
100%|██████████| 18/18 [02:19<00:00,  6.98s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 96%|█████████▌| 91/95 [5:16:08<16:05, 241.28s/it]                                                   96%|█████████▌| 91/95 [5:16:08<16:05, 241.28s/it] 97%|█████████▋| 92/95 [5:19:27<11:25, 228.35s/it]                                                   97%|█████████▋| 92/95 [5:19:27<11:25, 228.35s/it] 98%|█████████▊| 93/95 [5:22:45<07:18, 219.31s/it]                                                   98%|█████████▊| 93/95 [5:22:45<07:18, 219.31s/it] 99%|█████████▉| 94/95 [5:25:48<03:28, 208.45s/it]                                                   99%|█████████▉| 94/95 [5:25:48<03:28, 208.45s/it]100%|██████████| 95/95 [5:29:06<00:00, 205.43s/it]                                                  100%|██████████| 95/95 [5:29:06<00:00, 205.43s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                                  100%|██████████| 95/95 [5:29:13<00:00, 205.43s/it]100%|██████████| 95/95 [5:29:13<00:00, 207.93s/it]
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
{'eval_loss': 0.29117777943611145, 'eval_runtime': 145.5448, 'eval_samples_per_second': 0.941, 'eval_steps_per_second': 0.124, 'epoch': 4.62}
{'loss': 0.2696, 'grad_norm': 0.1211790218949318, 'learning_rate': 1.0908391628854041e-06, 'epoch': 4.67}
{'loss': 0.2691, 'grad_norm': 0.12121207267045975, 'learning_rate': 6.140863104726391e-07, 'epoch': 4.72}
{'loss': 0.2553, 'grad_norm': 0.1101093664765358, 'learning_rate': 2.7308266142119785e-07, 'epoch': 4.77}
{'loss': 0.359, 'grad_norm': 0.16607476770877838, 'learning_rate': 6.829398569770939e-08, 'epoch': 4.82}
{'loss': 0.2677, 'grad_norm': 0.11304137855768204, 'learning_rate': 0.0, 'epoch': 4.87}
{'train_runtime': 19756.1892, 'train_samples_per_second': 0.312, 'train_steps_per_second': 0.005, 'train_loss': 0.35393853407157094, 'epoch': 4.87}
wandb: - 0.013 MB of 0.013 MB uploadedwandb: \ 0.013 MB of 0.048 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss █▄▃▂▂▁▁▁▁
wandb:            eval/runtime ▁▅▆▅▃█▇▅▁
wandb: eval/samples_per_second ▁▁▁▁▁▁▁▁▁
wandb:   eval/steps_per_second ▁▁▁▁▁▁▁▁▁
wandb:             train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:       train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████
wandb:         train/grad_norm █▆▂▃▃▆▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁
wandb:     train/learning_rate ▂▃▅▇██████▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁
wandb:              train/loss █▇▆▇▄▃▂▃▆▂▂▂▂▄▁▂▅▂▂▂▂▃▁▂▂▂▃▂▁▁▁▃▄▂▃▁▁▄▁▁
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.29118
wandb:             eval/runtime 145.5448
wandb:  eval/samples_per_second 0.941
wandb:    eval/steps_per_second 0.124
wandb:               total_flos 2.7561011185761485e+17
wandb:              train/epoch 4.87179
wandb:        train/global_step 95
wandb:          train/grad_norm 0.11304
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.2677
wandb:               train_loss 0.35394
wandb:            train_runtime 19756.1892
wandb: train_samples_per_second 0.312
wandb:   train_steps_per_second 0.005
wandb: 
wandb: 🚀 View run finetune-5epoch at: https://wandb.ai/cx9/llama-dft/runs/dc10zev3
wandb: ⭐️ View project at: https://wandb.ai/cx9/llama-dft
wandb: Synced 6 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240717_010938-dc10zev3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
