current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
12
Thu Jul 18 23:34:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:07:00.0 Off |                    0 |
|  0%   36C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   39C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   33C    P0              71W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   38C    P0              71W / 300W |      4MiB / 46068MiB |      4%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [01:00<03:02, 60.91s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:01<03:04, 61.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:01<03:04, 61.39s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [01:01<03:04, 61.39s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:58<01:57, 58.67s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:58<01:57, 58.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:58<01:57, 58.87s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:58<01:57, 58.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:54<00:57, 57.87s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:55<00:57, 57.97s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:55<00:57, 57.98s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:55<00:57, 57.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 39.08s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 46.30s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 38.97s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 46.31s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 38.98s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 46.32s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 39.00s/it]Loading checkpoint shards: 100%|██████████| 4/4 [03:05<00:00, 46.33s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama3-vasp/wandb/run-20240718_233828-3cpmrgbz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-4gpu
wandb: ⭐️ View project at https://wandb.ai/cx9/llama-dft
wandb: 🚀 View run at https://wandb.ai/cx9/llama-dft/runs/3cpmrgbz
  0%|          | 0/60 [00:00<?, ?it/s]  2%|▏         | 1/60 [01:26<1:24:45, 86.20s/it]                                                {'loss': 0.6132, 'grad_norm': 1.6018191576004028, 'learning_rate': 2e-05, 'epoch': 0.08}
  2%|▏         | 1/60 [01:26<1:24:45, 86.20s/it]  3%|▎         | 2/60 [02:16<1:02:47, 64.96s/it]                                                {'loss': 0.8168, 'grad_norm': 1.8662406206130981, 'learning_rate': 4e-05, 'epoch': 0.15}
  3%|▎         | 2/60 [02:16<1:02:47, 64.96s/it]  5%|▌         | 3/60 [02:44<45:47, 48.20s/it]                                                {'loss': 0.9719, 'grad_norm': 1.9731836318969727, 'learning_rate': 6e-05, 'epoch': 0.23}
  5%|▌         | 3/60 [02:44<45:47, 48.20s/it]  7%|▋         | 4/60 [04:06<57:19, 61.42s/it]                                              {'loss': 0.5118, 'grad_norm': 0.814573347568512, 'learning_rate': 8e-05, 'epoch': 0.31}
  7%|▋         | 4/60 [04:06<57:19, 61.42s/it]  8%|▊         | 5/60 [04:51<51:00, 55.64s/it]                                              {'loss': 0.6031, 'grad_norm': 1.2871865034103394, 'learning_rate': 0.0001, 'epoch': 0.39}
  8%|▊         | 5/60 [04:51<51:00, 55.64s/it] 10%|█         | 6/60 [05:16<40:46, 45.30s/it]                                              {'loss': 0.7374, 'grad_norm': 2.9055449962615967, 'learning_rate': 0.00012, 'epoch': 0.46}
 10%|█         | 6/60 [05:16<40:46, 45.30s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.57s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.17s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                              
                                               [A{'eval_loss': 0.5111927390098572, 'eval_runtime': 41.9285, 'eval_samples_per_second': 3.267, 'eval_steps_per_second': 0.429, 'epoch': 0.46}
 10%|█         | 6/60 [05:58<40:46, 45.30s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A 12%|█▏        | 7/60 [07:20<1:02:40, 70.96s/it]                                                {'loss': 0.4288, 'grad_norm': 0.5843230485916138, 'learning_rate': 0.00014, 'epoch': 0.54}
 12%|█▏        | 7/60 [07:20<1:02:40, 70.96s/it] 13%|█▎        | 8/60 [08:05<54:19, 62.67s/it]                                                {'loss': 0.5206, 'grad_norm': 0.5960941910743713, 'learning_rate': 0.00016, 'epoch': 0.62}
 13%|█▎        | 8/60 [08:05<54:19, 62.67s/it] 15%|█▌        | 9/60 [08:32<43:38, 51.35s/it]                                              {'loss': 0.6138, 'grad_norm': 0.8345946669578552, 'learning_rate': 0.00018, 'epoch': 0.7}
 15%|█▌        | 9/60 [08:32<43:38, 51.35s/it] 17%|█▋        | 10/60 [09:57<51:35, 61.91s/it]                                               {'loss': 0.4116, 'grad_norm': 0.5644105076789856, 'learning_rate': 0.0002, 'epoch': 0.77}
 17%|█▋        | 10/60 [09:57<51:35, 61.91s/it] 18%|█▊        | 11/60 [10:45<47:06, 57.69s/it]                                               {'loss': 0.4749, 'grad_norm': 0.6526042819023132, 'learning_rate': 0.00019980267284282717, 'epoch': 0.85}
 18%|█▊        | 11/60 [10:45<47:06, 57.69s/it] 20%|██        | 12/60 [11:12<38:34, 48.22s/it]                                               {'loss': 0.5753, 'grad_norm': 0.31851550936698914, 'learning_rate': 0.0001992114701314478, 'epoch': 0.93}
 20%|██        | 12/60 [11:12<38:34, 48.22s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.57s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                               
                                               [A{'eval_loss': 0.4309544861316681, 'eval_runtime': 41.9314, 'eval_samples_per_second': 3.267, 'eval_steps_per_second': 0.429, 'epoch': 0.93}
 20%|██        | 12/60 [11:54<38:34, 48.22s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 22%|██▏       | 13/60 [13:04<52:58, 67.62s/it]                                               {'loss': 0.4087, 'grad_norm': 0.29683828353881836, 'learning_rate': 0.0001982287250728689, 'epoch': 1.01}
 22%|██▏       | 13/60 [13:04<52:58, 67.62s/it] 23%|██▎       | 14/60 [14:26<55:11, 71.99s/it]                                               {'loss': 0.3901, 'grad_norm': 0.2172258347272873, 'learning_rate': 0.0001968583161128631, 'epoch': 1.08}
 23%|██▎       | 14/60 [14:26<55:11, 71.99s/it] 25%|██▌       | 15/60 [15:12<48:00, 64.01s/it]                                               {'loss': 0.4216, 'grad_norm': 0.2918793559074402, 'learning_rate': 0.00019510565162951537, 'epoch': 1.16}
 25%|██▌       | 15/60 [15:12<48:00, 64.01s/it] 27%|██▋       | 16/60 [15:43<39:46, 54.24s/it]                                               {'loss': 0.4566, 'grad_norm': 0.2947366535663605, 'learning_rate': 0.00019297764858882514, 'epoch': 1.24}
 27%|██▋       | 16/60 [15:43<39:46, 54.24s/it] 28%|██▊       | 17/60 [17:01<44:02, 61.46s/it]                                               {'loss': 0.3834, 'grad_norm': 0.25713270902633667, 'learning_rate': 0.00019048270524660196, 'epoch': 1.32}
 28%|██▊       | 17/60 [17:01<44:02, 61.46s/it] 30%|███       | 18/60 [17:45<39:18, 56.15s/it]                                               {'loss': 0.427, 'grad_norm': 0.23622004687786102, 'learning_rate': 0.00018763066800438636, 'epoch': 1.39}
 30%|███       | 18/60 [17:45<39:18, 56.15s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.57s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                               
                                               [A{'eval_loss': 0.40170153975486755, 'eval_runtime': 41.9427, 'eval_samples_per_second': 3.266, 'eval_steps_per_second': 0.429, 'epoch': 1.39}
 30%|███       | 18/60 [18:27<39:18, 56.15s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A 32%|███▏      | 19/60 [18:58<41:46, 61.12s/it]                                               {'loss': 0.4991, 'grad_norm': 0.25337591767311096, 'learning_rate': 0.00018443279255020152, 'epoch': 1.47}
 32%|███▏      | 19/60 [18:58<41:46, 61.12s/it] 33%|███▎      | 20/60 [20:22<45:15, 67.88s/it]                                               {'loss': 0.3366, 'grad_norm': 0.24885673820972443, 'learning_rate': 0.00018090169943749476, 'epoch': 1.55}
 33%|███▎      | 20/60 [20:22<45:15, 67.88s/it] 35%|███▌      | 21/60 [21:08<40:02, 61.59s/it]                                               {'loss': 0.4504, 'grad_norm': 0.3169116973876953, 'learning_rate': 0.00017705132427757895, 'epoch': 1.63}
 35%|███▌      | 21/60 [21:08<40:02, 61.59s/it] 37%|███▋      | 22/60 [21:40<33:21, 52.67s/it]                                               {'loss': 0.4954, 'grad_norm': 0.24112863838672638, 'learning_rate': 0.00017289686274214118, 'epoch': 1.7}
 37%|███▋      | 22/60 [21:40<33:21, 52.67s/it] 38%|███▊      | 23/60 [23:02<37:52, 61.42s/it]                                               {'loss': 0.3637, 'grad_norm': 0.2446983903646469, 'learning_rate': 0.00016845471059286887, 'epoch': 1.78}
 38%|███▊      | 23/60 [23:02<37:52, 61.42s/it] 40%|████      | 24/60 [23:45<33:29, 55.81s/it]                                               {'loss': 0.4221, 'grad_norm': 0.185670405626297, 'learning_rate': 0.000163742398974869, 'epoch': 1.86}
 40%|████      | 24/60 [23:45<33:29, 55.81s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.23s/it][A                                               
                                               [A{'eval_loss': 0.3843609392642975, 'eval_runtime': 41.9542, 'eval_samples_per_second': 3.265, 'eval_steps_per_second': 0.429, 'epoch': 1.86}
 40%|████      | 24/60 [24:27<33:29, 55.81s/it]
100%|██████████| 18/18 [00:40<00:00,  2.23s/it][A
                                               [A 42%|████▏     | 25/60 [24:57<35:22, 60.63s/it]                                               {'loss': 0.5024, 'grad_norm': 0.2624737620353699, 'learning_rate': 0.00015877852522924732, 'epoch': 1.94}
 42%|████▏     | 25/60 [24:57<35:22, 60.63s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 43%|████▎     | 26/60 [26:08<36:12, 63.89s/it]                                               {'loss': 0.355, 'grad_norm': 0.15727461874485016, 'learning_rate': 0.00015358267949789966, 'epoch': 2.01}
 43%|████▎     | 26/60 [26:08<36:12, 63.89s/it] 45%|████▌     | 27/60 [27:30<38:09, 69.38s/it]                                               {'loss': 0.3291, 'grad_norm': 0.15263225138187408, 'learning_rate': 0.00014817536741017152, 'epoch': 2.09}
 45%|████▌     | 27/60 [27:30<38:09, 69.38s/it] 47%|████▋     | 28/60 [28:17<33:24, 62.64s/it]                                               {'loss': 0.3965, 'grad_norm': 0.17454397678375244, 'learning_rate': 0.00014257792915650728, 'epoch': 2.17}
 47%|████▋     | 28/60 [28:17<33:24, 62.64s/it] 48%|████▊     | 29/60 [28:55<28:31, 55.22s/it]                                               {'loss': 0.4207, 'grad_norm': 0.19536705315113068, 'learning_rate': 0.00013681245526846783, 'epoch': 2.25}
 48%|████▊     | 29/60 [28:55<28:31, 55.22s/it] 50%|█████     | 30/60 [30:12<30:48, 61.62s/it]                                               {'loss': 0.3611, 'grad_norm': 0.21995456516742706, 'learning_rate': 0.00013090169943749476, 'epoch': 2.32}
 50%|█████     | 30/60 [30:12<30:48, 61.62s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                               
                                               [A{'eval_loss': 0.37381258606910706, 'eval_runtime': 41.9456, 'eval_samples_per_second': 3.266, 'eval_steps_per_second': 0.429, 'epoch': 2.32}
 50%|█████     | 30/60 [30:54<30:48, 61.62s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A 52%|█████▏    | 31/60 [31:34<32:49, 67.91s/it]                                               {'loss': 0.3851, 'grad_norm': 0.18230146169662476, 'learning_rate': 0.0001248689887164855, 'epoch': 2.4}
 52%|█████▏    | 31/60 [31:34<32:49, 67.91s/it] 53%|█████▎    | 32/60 [32:10<27:10, 58.25s/it]                                               {'loss': 0.4306, 'grad_norm': 0.17679882049560547, 'learning_rate': 0.00011873813145857249, 'epoch': 2.48}
 53%|█████▎    | 32/60 [32:10<27:10, 58.25s/it] 55%|█████▌    | 33/60 [33:28<28:50, 64.09s/it]                                               {'loss': 0.3449, 'grad_norm': 0.15052229166030884, 'learning_rate': 0.00011253332335643043, 'epoch': 2.55}
 55%|█████▌    | 33/60 [33:28<28:50, 64.09s/it] 57%|█████▋    | 34/60 [34:11<25:05, 57.90s/it]                                               {'loss': 0.363, 'grad_norm': 0.15977059304714203, 'learning_rate': 0.00010627905195293135, 'epoch': 2.63}
 57%|█████▋    | 34/60 [34:11<25:05, 57.90s/it] 58%|█████▊    | 35/60 [34:48<21:25, 51.43s/it]                                               {'loss': 0.4101, 'grad_norm': 0.19989420473575592, 'learning_rate': 0.0001, 'epoch': 2.71}
 58%|█████▊    | 35/60 [34:48<21:25, 51.43s/it] 60%|██████    | 36/60 [36:06<23:48, 59.53s/it]                                               {'loss': 0.3466, 'grad_norm': 0.12786266207695007, 'learning_rate': 9.372094804706867e-05, 'epoch': 2.79}
 60%|██████    | 36/60 [36:06<23:48, 59.53s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.23s/it][A                                               
                                               [A{'eval_loss': 0.36353352665901184, 'eval_runtime': 41.9523, 'eval_samples_per_second': 3.266, 'eval_steps_per_second': 0.429, 'epoch': 2.79}
 60%|██████    | 36/60 [36:48<23:48, 59.53s/it]
100%|██████████| 18/18 [00:40<00:00,  2.23s/it][A
                                               [A 62%|██████▏   | 37/60 [37:29<25:32, 66.65s/it]                                               {'loss': 0.387, 'grad_norm': 0.18536774814128876, 'learning_rate': 8.746667664356956e-05, 'epoch': 2.86}
 62%|██████▏   | 37/60 [37:29<25:32, 66.65s/it] 63%|██████▎   | 38/60 [38:04<20:57, 57.15s/it]                                               {'loss': 0.4198, 'grad_norm': 0.1614585518836975, 'learning_rate': 8.126186854142752e-05, 'epoch': 2.94}
 63%|██████▎   | 38/60 [38:04<20:57, 57.15s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 65%|██████▌   | 39/60 [39:14<21:18, 60.88s/it]                                               {'loss': 0.3277, 'grad_norm': 0.12466691434383392, 'learning_rate': 7.513101128351454e-05, 'epoch': 3.02}
 65%|██████▌   | 39/60 [39:14<21:18, 60.88s/it] 67%|██████▋   | 40/60 [40:26<21:27, 64.38s/it]                                               {'loss': 0.3416, 'grad_norm': 0.1337425261735916, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.1}
 67%|██████▋   | 40/60 [40:26<21:27, 64.38s/it] 68%|██████▊   | 41/60 [41:06<17:59, 56.81s/it]                                               {'loss': 0.3626, 'grad_norm': 0.17039287090301514, 'learning_rate': 6.318754473153221e-05, 'epoch': 3.17}
 68%|██████▊   | 41/60 [41:06<17:59, 56.81s/it] 70%|███████   | 42/60 [41:46<15:34, 51.92s/it]                                               {'loss': 0.3821, 'grad_norm': 0.15969961881637573, 'learning_rate': 5.7422070843492734e-05, 'epoch': 3.25}
 70%|███████   | 42/60 [41:46<15:34, 51.92s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                               
                                               [A{'eval_loss': 0.3565821945667267, 'eval_runtime': 41.9548, 'eval_samples_per_second': 3.265, 'eval_steps_per_second': 0.429, 'epoch': 3.25}
 70%|███████   | 42/60 [42:28<15:34, 51.92s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A 72%|███████▏  | 43/60 [43:47<20:34, 72.59s/it]                                               {'loss': 0.3255, 'grad_norm': 0.11140479892492294, 'learning_rate': 5.182463258982846e-05, 'epoch': 3.33}
 72%|███████▏  | 43/60 [43:47<20:34, 72.59s/it] 73%|███████▎  | 44/60 [44:28<16:52, 63.29s/it]                                               {'loss': 0.348, 'grad_norm': 0.15421029925346375, 'learning_rate': 4.6417320502100316e-05, 'epoch': 3.41}
 73%|███████▎  | 44/60 [44:28<16:52, 63.29s/it] 75%|███████▌  | 45/60 [45:09<14:07, 56.47s/it]                                               {'loss': 0.3806, 'grad_norm': 0.1663644015789032, 'learning_rate': 4.12214747707527e-05, 'epoch': 3.48}
 75%|███████▌  | 45/60 [45:09<14:07, 56.47s/it] 77%|███████▋  | 46/60 [46:25<14:33, 62.41s/it]                                               {'loss': 0.3291, 'grad_norm': 0.129289910197258, 'learning_rate': 3.6257601025131026e-05, 'epoch': 3.56}
 77%|███████▋  | 46/60 [46:25<14:33, 62.41s/it] 78%|███████▊  | 47/60 [47:08<12:14, 56.47s/it]                                               {'loss': 0.3625, 'grad_norm': 0.14131729304790497, 'learning_rate': 3.154528940713113e-05, 'epoch': 3.64}
 78%|███████▊  | 47/60 [47:08<12:14, 56.47s/it] 80%|████████  | 48/60 [47:50<10:25, 52.15s/it]                                               {'loss': 0.355, 'grad_norm': 0.15026243031024933, 'learning_rate': 2.7103137257858868e-05, 'epoch': 3.72}
 80%|████████  | 48/60 [47:50<10:25, 52.15s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                               
                                               [A{'eval_loss': 0.35336512327194214, 'eval_runtime': 41.9489, 'eval_samples_per_second': 3.266, 'eval_steps_per_second': 0.429, 'epoch': 3.72}
 80%|████████  | 48/60 [48:32<10:25, 52.15s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A 82%|████████▏ | 49/60 [49:49<13:14, 72.26s/it]                                               {'loss': 0.3105, 'grad_norm': 0.11247270554304123, 'learning_rate': 2.2948675722421086e-05, 'epoch': 3.79}
 82%|████████▏ | 49/60 [49:49<13:14, 72.26s/it] 83%|████████▎ | 50/60 [50:31<10:30, 63.01s/it]                                               {'loss': 0.357, 'grad_norm': 0.12740489840507507, 'learning_rate': 1.9098300562505266e-05, 'epoch': 3.87}
 83%|████████▎ | 50/60 [50:31<10:30, 63.01s/it] 85%|████████▌ | 51/60 [51:10<08:22, 55.87s/it]                                               {'loss': 0.4096, 'grad_norm': 0.15228761732578278, 'learning_rate': 1.5567207449798515e-05, 'epoch': 3.95}
 85%|████████▌ | 51/60 [51:10<08:22, 55.87s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 87%|████████▋ | 52/60 [52:19<07:57, 59.72s/it]                                               {'loss': 0.3001, 'grad_norm': 0.11038149148225784, 'learning_rate': 1.2369331995613665e-05, 'epoch': 4.03}
 87%|████████▋ | 52/60 [52:19<07:57, 59.72s/it] 88%|████████▊ | 53/60 [53:29<07:20, 62.92s/it]                                               {'loss': 0.3638, 'grad_norm': 0.1032293438911438, 'learning_rate': 9.517294753398064e-06, 'epoch': 4.1}
 88%|████████▊ | 53/60 [53:29<07:20, 62.92s/it] 90%|█████████ | 54/60 [54:09<05:36, 56.06s/it]                                               {'loss': 0.3547, 'grad_norm': 0.12310606241226196, 'learning_rate': 7.022351411174866e-06, 'epoch': 4.18}
 90%|█████████ | 54/60 [54:09<05:36, 56.06s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.23s/it][A                                               
                                               [A{'eval_loss': 0.3512524664402008, 'eval_runtime': 41.9316, 'eval_samples_per_second': 3.267, 'eval_steps_per_second': 0.429, 'epoch': 4.18}
 90%|█████████ | 54/60 [54:51<05:36, 56.06s/it]
100%|██████████| 18/18 [00:40<00:00,  2.23s/it][A
                                               [A 92%|█████████▏| 55/60 [55:37<05:27, 65.59s/it]                                               {'loss': 0.3465, 'grad_norm': 0.13570255041122437, 'learning_rate': 4.8943483704846475e-06, 'epoch': 4.26}
 92%|█████████▏| 55/60 [55:37<05:27, 65.59s/it] 93%|█████████▎| 56/60 [56:50<04:31, 67.87s/it]                                               {'loss': 0.3303, 'grad_norm': 0.10225071758031845, 'learning_rate': 3.1416838871368924e-06, 'epoch': 4.34}
 93%|█████████▎| 56/60 [56:50<04:31, 67.87s/it] 95%|█████████▌| 57/60 [57:30<02:58, 59.40s/it]                                               {'loss': 0.3319, 'grad_norm': 0.12070260941982269, 'learning_rate': 1.771274927131139e-06, 'epoch': 4.41}
 95%|█████████▌| 57/60 [57:30<02:58, 59.40s/it] 97%|█████████▋| 58/60 [58:15<01:50, 55.24s/it]                                               {'loss': 0.3649, 'grad_norm': 0.14624448120594025, 'learning_rate': 7.885298685522235e-07, 'epoch': 4.49}
 97%|█████████▋| 58/60 [58:15<01:50, 55.24s/it] 98%|█████████▊| 59/60 [59:27<01:00, 60.16s/it]                                               {'loss': 0.3164, 'grad_norm': 0.1088561937212944, 'learning_rate': 1.973271571728441e-07, 'epoch': 4.57}
 98%|█████████▊| 59/60 [59:27<01:00, 60.16s/it]100%|██████████| 60/60 [1:00:06<00:00, 53.89s/it]                                                 {'loss': 0.3396, 'grad_norm': 0.10782896727323532, 'learning_rate': 0.0, 'epoch': 4.65}
100%|██████████| 60/60 [1:00:06<00:00, 53.89s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.11s/it][A
 33%|███▎      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|███▉      | 7/18 [00:14<00:26,  2.37s/it][A
 44%|████▍     | 8/18 [00:17<00:24,  2.40s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.18s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.48s/it][A
 61%|██████    | 11/18 [00:24<00:16,  2.35s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.40s/it][A
 72%|███████▏  | 13/18 [00:28<00:10,  2.17s/it][A
 78%|███████▊  | 14/18 [00:29<00:08,  2.02s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.26s/it][A
 89%|████████▉ | 16/18 [00:35<00:04,  2.29s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.10s/it][A
100%|██████████| 18/18 [00:39<00:00,  2.24s/it][A                                                 
                                               [A{'eval_loss': 0.35109490156173706, 'eval_runtime': 41.9474, 'eval_samples_per_second': 3.266, 'eval_steps_per_second': 0.429, 'epoch': 4.65}
100%|██████████| 60/60 [1:00:48<00:00, 53.89s/it]
100%|██████████| 18/18 [00:40<00:00,  2.24s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                                 {'train_runtime': 3668.6051, 'train_samples_per_second': 1.68, 'train_steps_per_second': 0.016, 'train_loss': 0.42376502603292465, 'epoch': 4.65}
100%|██████████| 60/60 [1:01:03<00:00, 53.89s/it]100%|██████████| 60/60 [1:01:03<00:00, 61.06s/it]
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.008 MB of 0.008 MB uploadedwandb: | 0.042 MB of 0.070 MB uploaded (0.002 MB deduped)wandb: / 0.070 MB of 0.070 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:               eval/loss █▄▃▂▂▂▁▁▁▁
wandb:            eval/runtime ▁▂▅█▆▇█▆▂▆
wandb: eval/samples_per_second ██▅▁▅▅▁▅█▅
wandb:   eval/steps_per_second ▁▁▁▁▁▁▁▁▁▁
wandb:             train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:       train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:         train/grad_norm ▇█▄▆▃▃▃▃▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:     train/learning_rate ▂▂▄▅▆▇██████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:              train/loss ▅█▄▅▃▄▃▃▂▂▃▂▄▁▄▂▄▂▂▃▂▃▂▂▂▃▂▂▁▂▁▂▁▂▁▂▂▁▂▂
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.35109
wandb:             eval/runtime 41.9474
wandb:  eval/samples_per_second 3.266
wandb:    eval/steps_per_second 0.429
wandb:               total_flos 3.835766833634345e+17
wandb:              train/epoch 4.64516
wandb:        train/global_step 60
wandb:          train/grad_norm 0.10783
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.3396
wandb:               train_loss 0.42377
wandb:            train_runtime 3668.6051
wandb: train_samples_per_second 1.68
wandb:   train_steps_per_second 0.016
wandb: 
wandb: 🚀 View run finetune-4gpu at: https://wandb.ai/cx9/llama-dft/runs/3cpmrgbz
wandb: ⭐️ View project at: https://wandb.ai/cx9/llama-dft
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240718_233828-3cpmrgbz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
