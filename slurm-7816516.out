current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
12
Thu Jul 18 18:07:34 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:06:00.0 Off |                    0 |
|  0%   36C    P0              73W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   41C    P0              77W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   35C    P0              73W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   41C    P0              83W / 300W |      4MiB / 46068MiB |      3%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:44<02:14, 44.92s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:15, 45.33s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:16, 45.34s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:45<02:16, 45.35s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:24<01:23, 41.94s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.11s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.12s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:25<01:24, 42.15s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:05<00:41, 41.09s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:05<00:41, 41.36s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:05<00:41, 41.26s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:05<00:41, 41.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 28.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 33.45s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 28.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 33.47s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 28.30s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 33.47s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 28.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:13<00:00, 33.48s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama3-vasp/wandb/run-20240718_181025-hvhppedb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-4gpu
wandb: â­ï¸ View project at https://wandb.ai/cx9/llama-dft
wandb: ðŸš€ View run at https://wandb.ai/cx9/llama-dft/runs/hvhppedb
  0%|          | 0/95 [00:00<?, ?it/s]  1%|          | 1/95 [01:02<1:37:45, 62.40s/it]                                                {'loss': 0.5239, 'grad_norm': 2.2589871883392334, 'learning_rate': 2e-05, 'epoch': 0.05}
  1%|          | 1/95 [01:02<1:37:45, 62.40s/it]  2%|â–         | 2/95 [01:49<1:22:31, 53.24s/it]                                                {'loss': 0.7739, 'grad_norm': 3.0036568641662598, 'learning_rate': 4e-05, 'epoch': 0.1}
  2%|â–         | 2/95 [01:49<1:22:31, 53.24s/it]  3%|â–Ž         | 3/95 [02:20<1:06:04, 43.09s/it]                                                {'loss': 0.735, 'grad_norm': 2.1844959259033203, 'learning_rate': 6e-05, 'epoch': 0.15}
  3%|â–Ž         | 3/95 [02:20<1:06:04, 43.09s/it]  4%|â–         | 4/95 [02:42<52:56, 34.91s/it]                                                {'loss': 0.7449, 'grad_norm': 2.4285435676574707, 'learning_rate': 8e-05, 'epoch': 0.21}
  4%|â–         | 4/95 [02:42<52:56, 34.91s/it]  5%|â–Œ         | 5/95 [03:08<47:42, 31.80s/it]                                              {'loss': 0.7733, 'grad_norm': 2.0530829429626465, 'learning_rate': 0.0001, 'epoch': 0.26}
  5%|â–Œ         | 5/95 [03:08<47:42, 31.80s/it]  6%|â–‹         | 6/95 [04:04<59:15, 39.95s/it]                                              {'loss': 0.4619, 'grad_norm': 0.9156884551048279, 'learning_rate': 0.00012, 'epoch': 0.31}
  6%|â–‹         | 6/95 [04:04<59:15, 39.95s/it]  7%|â–‹         | 7/95 [04:40<56:25, 38.48s/it]                                              {'loss': 0.513, 'grad_norm': 1.4115582704544067, 'learning_rate': 0.00014, 'epoch': 0.36}
  7%|â–‹         | 7/95 [04:40<56:25, 38.48s/it]  8%|â–Š         | 8/95 [05:06<50:13, 34.64s/it]                                              {'loss': 0.5188, 'grad_norm': 1.0099564790725708, 'learning_rate': 0.00016, 'epoch': 0.41}
  8%|â–Š         | 8/95 [05:06<50:13, 34.64s/it]  9%|â–‰         | 9/95 [05:25<42:28, 29.64s/it]                                              {'loss': 0.5286, 'grad_norm': 0.7684957981109619, 'learning_rate': 0.00018, 'epoch': 0.46}
  9%|â–‰         | 9/95 [05:25<42:28, 29.64s/it] 11%|â–ˆ         | 10/95 [06:00<44:39, 31.53s/it]                                               {'loss': 0.4879, 'grad_norm': 0.9921122789382935, 'learning_rate': 0.0002, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:00<44:39, 31.53s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.15s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.62s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.07s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.10s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.24s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.36s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.16s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.46s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:28<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.27s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.46141937375068665, 'eval_runtime': 41.6143, 'eval_samples_per_second': 3.292, 'eval_steps_per_second': 0.433, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:42<44:39, 31.53s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 12%|â–ˆâ–        | 11/95 [07:42<1:14:15, 53.05s/it]                                                 {'loss': 0.4527, 'grad_norm': 0.48324230313301086, 'learning_rate': 0.0001999317060143023, 'epoch': 0.57}
 12%|â–ˆâ–        | 11/95 [07:42<1:14:15, 53.05s/it] 13%|â–ˆâ–Ž        | 12/95 [08:16<1:05:24, 47.28s/it]                                                 {'loss': 0.4832, 'grad_norm': 0.6584638357162476, 'learning_rate': 0.00019972691733857883, 'epoch': 0.62}
 13%|â–ˆâ–Ž        | 12/95 [08:16<1:05:24, 47.28s/it] 14%|â–ˆâ–Ž        | 13/95 [08:42<55:32, 40.63s/it]                                                 {'loss': 0.492, 'grad_norm': 1.3285952806472778, 'learning_rate': 0.0001993859136895274, 'epoch': 0.67}
 14%|â–ˆâ–Ž        | 13/95 [08:42<55:32, 40.63s/it] 15%|â–ˆâ–        | 14/95 [08:59<45:21, 33.60s/it]                                               {'loss': 0.4951, 'grad_norm': 0.6243408918380737, 'learning_rate': 0.0001989091608371146, 'epoch': 0.72}
 15%|â–ˆâ–        | 14/95 [08:59<45:21, 33.60s/it] 16%|â–ˆâ–Œ        | 15/95 [09:47<50:39, 37.99s/it]                                               {'loss': 0.4724, 'grad_norm': 0.7198337316513062, 'learning_rate': 0.0001982973099683902, 'epoch': 0.77}
 16%|â–ˆâ–Œ        | 15/95 [09:47<50:39, 37.99s/it] 17%|â–ˆâ–‹        | 16/95 [10:40<55:54, 42.47s/it]                                               {'loss': 0.4308, 'grad_norm': 0.41688692569732666, 'learning_rate': 0.00019755119679804367, 'epoch': 0.83}
 17%|â–ˆâ–‹        | 16/95 [10:40<55:54, 42.47s/it] 18%|â–ˆâ–Š        | 17/95 [11:12<51:15, 39.43s/it]                                               {'loss': 0.4537, 'grad_norm': 0.4747684597969055, 'learning_rate': 0.00019667184042691875, 'epoch': 0.88}
 18%|â–ˆâ–Š        | 17/95 [11:13<51:15, 39.43s/it] 19%|â–ˆâ–‰        | 18/95 [11:35<44:10, 34.42s/it]                                               {'loss': 0.5027, 'grad_norm': 0.42445632815361023, 'learning_rate': 0.0001956604419500441, 'epoch': 0.93}
 19%|â–ˆâ–‰        | 18/95 [11:35<44:10, 34.42s/it] 20%|â–ˆâ–ˆ        | 19/95 [11:51<36:20, 28.69s/it]                                               {'loss': 0.5511, 'grad_norm': 0.48474249243736267, 'learning_rate': 0.00019451838281608197, 'epoch': 0.98}
 20%|â–ˆâ–ˆ        | 19/95 [11:51<36:20, 28.69s/it] 21%|â–ˆâ–ˆ        | 20/95 [12:47<46:13, 36.98s/it]                                               {'loss': 0.3188, 'grad_norm': 0.2937629520893097, 'learning_rate': 0.00019324722294043558, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [12:47<46:13, 36.98s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.411422997713089, 'eval_runtime': 41.41, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.435, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [13:28<46:13, 36.98s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 21/95 [14:29<1:09:48, 56.61s/it]                                                 {'loss': 0.4139, 'grad_norm': 0.33203941583633423, 'learning_rate': 0.00019184869857459232, 'epoch': 1.08}
 22%|â–ˆâ–ˆâ–       | 21/95 [14:29<1:09:48, 56.61s/it] 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:04<1:01:01, 50.15s/it]                                                 {'loss': 0.4137, 'grad_norm': 0.32627788186073303, 'learning_rate': 0.0001903247199346129, 'epoch': 1.14}
 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:04<1:01:01, 50.15s/it] 24%|â–ˆâ–ˆâ–       | 23/95 [15:29<51:03, 42.55s/it]                                                 {'loss': 0.4547, 'grad_norm': 0.3040097951889038, 'learning_rate': 0.0001886773685920062, 'epoch': 1.19}
 24%|â–ˆâ–ˆâ–       | 23/95 [15:29<51:03, 42.55s/it] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:46<41:06, 34.74s/it]                                               {'loss': 0.4534, 'grad_norm': 0.39829787611961365, 'learning_rate': 0.00018690889463055283, 'epoch': 1.24}
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:46<41:06, 34.74s/it] 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:40<47:24, 40.63s/it]                                               {'loss': 0.3406, 'grad_norm': 0.2990185618400574, 'learning_rate': 0.00018502171357296144, 'epoch': 1.29}
 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:40<47:24, 40.63s/it] 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:27<48:53, 42.52s/it]                                               {'loss': 0.4355, 'grad_norm': 0.39525359869003296, 'learning_rate': 0.00018301840308155507, 'epoch': 1.34}
 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:27<48:53, 42.52s/it] 28%|â–ˆâ–ˆâ–Š       | 27/95 [17:57<43:59, 38.82s/it]                                               {'loss': 0.4251, 'grad_norm': 0.3200169801712036, 'learning_rate': 0.00018090169943749476, 'epoch': 1.39}
 28%|â–ˆâ–ˆâ–Š       | 27/95 [17:57<43:59, 38.82s/it] 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:20<37:50, 33.88s/it]                                               {'loss': 0.4227, 'grad_norm': 0.32655248045921326, 'learning_rate': 0.00017867449380334834, 'epoch': 1.45}
 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:20<37:50, 33.88s/it] 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:40<32:51, 29.87s/it]                                               {'loss': 0.4912, 'grad_norm': 0.3475418984889984, 'learning_rate': 0.00017633982827411032, 'epoch': 1.5}
 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:40<32:51, 29.87s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:39<41:53, 38.67s/it]                                               {'loss': 0.314, 'grad_norm': 0.3559523820877075, 'learning_rate': 0.00017390089172206592, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:39<41:53, 38.67s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.3826737701892853, 'eval_runtime': 41.4477, 'eval_samples_per_second': 3.305, 'eval_steps_per_second': 0.434, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [20:21<41:53, 38.67s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:07<56:49, 53.27s/it]                                               {'loss': 0.3848, 'grad_norm': 0.2731837332248688, 'learning_rate': 0.00017136101544117525, 'epoch': 1.6}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:07<56:49, 53.27s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:35<48:03, 45.77s/it]                                               {'loss': 0.3897, 'grad_norm': 0.28306248784065247, 'learning_rate': 0.00016872366859692627, 'epoch': 1.65}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:35<48:03, 45.77s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [21:55<39:24, 38.13s/it]                                               {'loss': 0.4058, 'grad_norm': 0.2593882381916046, 'learning_rate': 0.0001659924534878723, 'epoch': 1.7}
 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [21:55<39:24, 38.13s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:26<36:32, 35.95s/it]                                               {'loss': 0.4271, 'grad_norm': 0.2671661376953125, 'learning_rate': 0.0001631711006253251, 'epoch': 1.75}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:26<36:32, 35.95s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:22<42:01, 42.03s/it]                                               {'loss': 0.3204, 'grad_norm': 0.28103479743003845, 'learning_rate': 0.00016026346363792567, 'epoch': 1.81}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:22<42:01, 42.03s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [23:59<39:42, 40.38s/it]                                               {'loss': 0.4012, 'grad_norm': 0.30355286598205566, 'learning_rate': 0.00015727351400805052, 'epoch': 1.86}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [23:59<39:42, 40.38s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:25<34:49, 36.02s/it]                                               {'loss': 0.3896, 'grad_norm': 0.2552785277366638, 'learning_rate': 0.00015420533564724495, 'epoch': 1.91}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:25<34:49, 36.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:42<29:00, 30.54s/it]                                               {'loss': 0.4251, 'grad_norm': 0.3174045979976654, 'learning_rate': 0.0001510631193180907, 'epoch': 1.96}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:42<29:00, 30.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:20<30:24, 32.58s/it]                                               {'loss': 0.4239, 'grad_norm': 0.28756001591682434, 'learning_rate': 0.00014785115691012864, 'epoch': 2.01}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:20<30:24, 32.58s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:19<37:08, 40.51s/it]                                               {'loss': 0.2889, 'grad_norm': 0.20471526682376862, 'learning_rate': 0.00014457383557765386, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:19<37:08, 40.51s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.27s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.3666248023509979, 'eval_runtime': 41.4467, 'eval_samples_per_second': 3.305, 'eval_steps_per_second': 0.434, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [27:00<37:08, 40.51s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:47<49:17, 54.77s/it]                                               {'loss': 0.3641, 'grad_norm': 0.284993976354599, 'learning_rate': 0.00014123563174739037, 'epoch': 2.12}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:47<49:17, 54.77s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:13<40:53, 46.29s/it]                                               {'loss': 0.3844, 'grad_norm': 0.24492426216602325, 'learning_rate': 0.00013784110500423104, 'epoch': 2.17}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:13<40:53, 46.29s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:33<33:06, 38.20s/it]                                               {'loss': 0.3577, 'grad_norm': 0.2503170371055603, 'learning_rate': 0.00013439489186339282, 'epoch': 2.22}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:33<33:06, 38.20s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:09<31:59, 37.63s/it]                                               {'loss': 0.3566, 'grad_norm': 0.27906838059425354, 'learning_rate': 0.00013090169943749476, 'epoch': 2.27}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:09<31:59, 37.63s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [29:59<34:33, 41.48s/it]                                               {'loss': 0.3547, 'grad_norm': 0.2554454803466797, 'learning_rate': 0.0001273662990072083, 'epoch': 2.32}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [29:59<34:33, 41.48s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:32<31:40, 38.78s/it]                                               {'loss': 0.368, 'grad_norm': 0.2800728976726532, 'learning_rate': 0.00012379351950426187, 'epoch': 2.37}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:32<31:40, 38.78s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [30:56<27:35, 34.49s/it]                                               {'loss': 0.3874, 'grad_norm': 0.2507980167865753, 'learning_rate': 0.00012018824091570103, 'epoch': 2.43}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [30:56<27:35, 34.49s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:13<22:53, 29.22s/it]                                               {'loss': 0.3873, 'grad_norm': 0.276108980178833, 'learning_rate': 0.000116555387618413, 'epoch': 2.48}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:13<22:53, 29.22s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:01<26:41, 34.82s/it]                                               {'loss': 0.3515, 'grad_norm': 0.28036484122276306, 'learning_rate': 0.00011289992165302035, 'epoch': 2.53}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:01<26:41, 34.82s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [32:57<30:49, 41.11s/it]                                               {'loss': 0.3399, 'grad_norm': 0.23069393634796143, 'learning_rate': 0.00010922683594633021, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [32:57<30:49, 41.11s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.35499611496925354, 'eval_runtime': 41.4379, 'eval_samples_per_second': 3.306, 'eval_steps_per_second': 0.434, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:38<30:49, 41.11s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:21<39:34, 53.97s/it]                                               {'loss': 0.3437, 'grad_norm': 0.2732950747013092, 'learning_rate': 0.000105541147491597, 'epoch': 2.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:21<39:34, 53.97s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:45<32:16, 45.04s/it]                                               {'loss': 0.362, 'grad_norm': 0.2251594215631485, 'learning_rate': 0.00010184789049591299, 'epoch': 2.68}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:45<32:16, 45.04s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:00<25:15, 36.09s/it]                                               {'loss': 0.4509, 'grad_norm': 0.34326642751693726, 'learning_rate': 9.815210950408704e-05, 'epoch': 2.74}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:00<25:15, 36.09s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [35:59<29:19, 42.91s/it]                                               {'loss': 0.2573, 'grad_norm': 0.18356101214885712, 'learning_rate': 9.4458852508403e-05, 'epoch': 2.79}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [35:59<29:19, 42.91s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:39<27:54, 41.87s/it]                                               {'loss': 0.3534, 'grad_norm': 0.2474673092365265, 'learning_rate': 9.077316405366981e-05, 'epoch': 2.84}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:39<27:54, 41.87s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:07<24:38, 37.91s/it]                                               {'loss': 0.3349, 'grad_norm': 0.20602698624134064, 'learning_rate': 8.710007834697969e-05, 'epoch': 2.89}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:07<24:38, 37.91s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:28<20:50, 32.91s/it]                                               {'loss': 0.3793, 'grad_norm': 0.24986429512500763, 'learning_rate': 8.344461238158699e-05, 'epoch': 2.94}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:28<20:50, 32.91s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [37:51<18:24, 29.85s/it]                                               {'loss': 0.439, 'grad_norm': 0.27178725600242615, 'learning_rate': 7.9811759084299e-05, 'epoch': 2.99}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [37:51<18:24, 29.85s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [38:52<23:25, 39.04s/it]                                               {'loss': 0.2617, 'grad_norm': 0.14983516931533813, 'learning_rate': 7.620648049573815e-05, 'epoch': 3.05}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [38:52<23:25, 39.04s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:34<23:24, 40.14s/it]                                               {'loss': 0.3406, 'grad_norm': 0.21294909715652466, 'learning_rate': 7.263370099279172e-05, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:34<23:24, 40.14s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.34606510400772095, 'eval_runtime': 41.4129, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.435, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [40:16<23:24, 40.14s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [40:52<29:07, 51.40s/it]                                               {'loss': 0.3192, 'grad_norm': 0.20547014474868774, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.15}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [40:52<29:07, 51.40s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:14<23:27, 42.66s/it]                                               {'loss': 0.347, 'grad_norm': 0.21966060996055603, 'learning_rate': 6.560510813660719e-05, 'epoch': 3.2}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:14<23:27, 42.66s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:35<19:12, 36.02s/it]                                               {'loss': 0.3631, 'grad_norm': 0.27264314889907837, 'learning_rate': 6.215889499576898e-05, 'epoch': 3.25}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:35<19:12, 36.02s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:35<22:23, 43.33s/it]                                               {'loss': 0.235, 'grad_norm': 0.16726532578468323, 'learning_rate': 5.876436825260967e-05, 'epoch': 3.3}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:35<22:23, 43.33s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:17<21:25, 42.83s/it]                                               {'loss': 0.3562, 'grad_norm': 0.26421886682510376, 'learning_rate': 5.542616442234618e-05, 'epoch': 3.35}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:17<21:25, 42.83s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:45<18:30, 38.28s/it]                                               {'loss': 0.3313, 'grad_norm': 0.24525560438632965, 'learning_rate': 5.214884308987136e-05, 'epoch': 3.41}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:45<18:30, 38.28s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:05<15:23, 32.99s/it]                                               {'loss': 0.3629, 'grad_norm': 0.21590793132781982, 'learning_rate': 4.893688068190932e-05, 'epoch': 3.46}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:05<15:23, 32.99s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:36<14:33, 32.36s/it]                                               {'loss': 0.3721, 'grad_norm': 0.2723507583141327, 'learning_rate': 4.5794664352755055e-05, 'epoch': 3.51}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:36<14:33, 32.36s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:32<17:02, 39.33s/it]                                               {'loss': 0.2803, 'grad_norm': 0.14796625077724457, 'learning_rate': 4.272648599194948e-05, 'epoch': 3.56}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:32<17:02, 39.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:09<16:05, 38.61s/it]                                               {'loss': 0.309, 'grad_norm': 0.18483440577983856, 'learning_rate': 3.973653636207437e-05, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:09<16:05, 38.61s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.3417350947856903, 'eval_runtime': 41.4434, 'eval_samples_per_second': 3.306, 'eval_steps_per_second': 0.434, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:50<16:05, 38.61s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:22<19:38, 49.11s/it]                                               {'loss': 0.3404, 'grad_norm': 0.21901413798332214, 'learning_rate': 3.682889937467493e-05, 'epoch': 3.66}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:22<19:38, 49.11s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:40<15:14, 39.74s/it]                                               {'loss': 0.3297, 'grad_norm': 0.24459443986415863, 'learning_rate': 3.400754651212776e-05, 'epoch': 3.72}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:40<15:14, 39.74s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:22<14:48, 40.39s/it]                                               {'loss': 0.2844, 'grad_norm': 0.32413333654403687, 'learning_rate': 3.1276331403073735e-05, 'epoch': 3.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:22<14:48, 40.39s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:16<15:37, 44.62s/it]                                               {'loss': 0.2986, 'grad_norm': 0.17179076373577118, 'learning_rate': 2.8638984558824777e-05, 'epoch': 3.82}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:16<15:37, 44.62s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [49:52<13:56, 41.84s/it]                                               {'loss': 0.3303, 'grad_norm': 0.17322300374507904, 'learning_rate': 2.6099108277934103e-05, 'epoch': 3.87}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [49:52<13:56, 41.84s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:17<11:37, 36.72s/it]                                               {'loss': 0.3468, 'grad_norm': 0.2197059839963913, 'learning_rate': 2.36601717258897e-05, 'epoch': 3.92}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:17<11:37, 36.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:33<09:10, 30.58s/it]                                               {'loss': 0.4074, 'grad_norm': 0.25423890352249146, 'learning_rate': 2.132550619665168e-05, 'epoch': 3.97}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:33<09:10, 30.58s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:23<10:21, 36.54s/it]                                               {'loss': 0.2694, 'grad_norm': 0.19224412739276886, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.03}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:23<10:21, 36.54s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:18<11:12, 42.04s/it]                                               {'loss': 0.2572, 'grad_norm': 0.14668793976306915, 'learning_rate': 1.6981596918444953e-05, 'epoch': 4.08}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:18<11:12, 42.04s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [52:52<09:53, 39.58s/it]                                               {'loss': 0.3219, 'grad_norm': 0.1680901050567627, 'learning_rate': 1.4978286427038601e-05, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [52:52<09:53, 39.58s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.3389184772968292, 'eval_runtime': 41.4486, 'eval_samples_per_second': 3.305, 'eval_steps_per_second': 0.434, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [53:33<09:53, 39.58s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:04<11:31, 49.36s/it]                                               {'loss': 0.3323, 'grad_norm': 0.17556795477867126, 'learning_rate': 1.3091105369447165e-05, 'epoch': 4.18}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:04<11:31, 49.36s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:22<08:37, 39.81s/it]                                               {'loss': 0.2867, 'grad_norm': 0.23488056659698486, 'learning_rate': 1.1322631407993811e-05, 'epoch': 4.23}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:22<08:37, 39.81s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:10<08:28, 42.36s/it]                                               {'loss': 0.3013, 'grad_norm': 0.20772361755371094, 'learning_rate': 9.675280065387116e-06, 'epoch': 4.28}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:10<08:28, 42.36s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:01<08:13, 44.90s/it]                                               {'loss': 0.3182, 'grad_norm': 0.14961189031600952, 'learning_rate': 8.151301425407699e-06, 'epoch': 4.34}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:01<08:13, 44.90s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:33<06:51, 41.18s/it]                                               {'loss': 0.3112, 'grad_norm': 0.17475205659866333, 'learning_rate': 6.75277705956443e-06, 'epoch': 4.39}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:33<06:51, 41.18s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [56:56<05:21, 35.71s/it]                                               {'loss': 0.3292, 'grad_norm': 0.196921244263649, 'learning_rate': 5.481617183918053e-06, 'epoch': 4.44}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [56:56<05:21, 35.71s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:11<03:54, 29.30s/it]                                               {'loss': 0.3905, 'grad_norm': 0.33467310667037964, 'learning_rate': 4.339558049955927e-06, 'epoch': 4.49}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:11<03:54, 29.30s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:10<04:27, 38.24s/it]                                               {'loss': 0.2518, 'grad_norm': 0.15364843606948853, 'learning_rate': 3.3281595730812575e-06, 'epoch': 4.54}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:10<04:27, 38.24s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [58:51<03:55, 39.25s/it]                                               {'loss': 0.3201, 'grad_norm': 0.16881045699119568, 'learning_rate': 2.4488032019563402e-06, 'epoch': 4.59}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [58:51<03:55, 39.25s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:21<03:02, 36.46s/it]                                               {'loss': 0.3107, 'grad_norm': 0.1761966049671173, 'learning_rate': 1.7026900316098215e-06, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:21<03:02, 36.46s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:12<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.38s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.21s/it][A                                               
                                               [A{'eval_loss': 0.34028947353363037, 'eval_runtime': 41.4524, 'eval_samples_per_second': 3.305, 'eval_steps_per_second': 0.434, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [1:00:03<03:02, 36.46s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.21s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:32<03:06, 46.61s/it]                                                 {'loss': 0.3207, 'grad_norm': 0.20245857536792755, 'learning_rate': 1.0908391628854041e-06, 'epoch': 4.7}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:32<03:06, 46.61s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:00:57<02:00, 40.23s/it]                                                 {'loss': 0.3439, 'grad_norm': 0.3454980254173279, 'learning_rate': 6.140863104726391e-07, 'epoch': 4.75}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:00:57<02:00, 40.23s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:01:56<01:31, 45.94s/it]                                                 {'loss': 0.258, 'grad_norm': 0.13577334582805634, 'learning_rate': 2.7308266142119785e-07, 'epoch': 4.8}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:01:56<01:31, 45.94s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:36<00:44, 44.22s/it]                                                 {'loss': 0.3205, 'grad_norm': 0.17336241900920868, 'learning_rate': 6.829398569770939e-08, 'epoch': 4.85}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:36<00:44, 44.22s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:03<00:00, 38.93s/it]                                                 {'loss': 0.2962, 'grad_norm': 0.19416822493076324, 'learning_rate': 0.0, 'epoch': 4.9}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:03<00:00, 38.93s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank2]: Traceback (most recent call last):
[rank2]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank2]:     output = super().train(*args, **kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank2]:     self._load_best_model()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank2]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank2]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank2]:     adapters_weights = safe_load_file(filename, device=device)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank2]:     result[k] = f.get_tensor(k)
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank3]: Traceback (most recent call last):
[rank3]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank3]:     trainer.train()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank3]:     output = super().train(*args, **kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank3]:     self._load_best_model()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank3]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank3]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank3]:     adapters_weights = safe_load_file(filename, device=device)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank3]:     result[k] = f.get_tensor(k)
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank1]: Traceback (most recent call last):
[rank1]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank1]:     output = super().train(*args, **kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank1]:     self._load_best_model()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank1]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank1]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank1]:     adapters_weights = safe_load_file(filename, device=device)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank1]:     result[k] = f.get_tensor(k)
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
W0718 19:13:49.028000 23022427832832 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 534765 closing signal SIGTERM
W0718 19:13:49.028000 23022427832832 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 534766 closing signal SIGTERM
W0718 19:13:49.028000 23022427832832 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 534767 closing signal SIGTERM
E0718 19:13:49.844000 23022427832832 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 3 (pid: 534768) of binary: /global/scratch/users/chenxin0210/conda-env/llm/bin/python
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/conda-env/llm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run-llama.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-18_19:13:49
  host      : n0048.es1
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 534768)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
