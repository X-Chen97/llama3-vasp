current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
12
Fri Jul 19 19:05:40 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:06:00.0 Off |                    0 |
|  0%   36C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   44C    P0              80W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   37C    P0              74W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   41C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:44<02:14, 44.98s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:16, 45.40s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:16, 45.41s/it]Loading checkpoint shards:  25%|██▌       | 1/4 [00:45<02:16, 45.42s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:24<01:22, 41.43s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:24<01:23, 41.76s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:24<01:23, 41.69s/it]Loading checkpoint shards:  50%|█████     | 2/4 [01:24<01:23, 41.69s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:02<00:39, 39.92s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:02<00:40, 40.06s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:02<00:40, 40.07s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [02:02<00:40, 40.13s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 27.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 32.72s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 27.54s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 32.74s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 27.55s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 32.75s/it]
Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 27.59s/it]Loading checkpoint shards: 100%|██████████| 4/4 [02:10<00:00, 32.75s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: wandb version 0.17.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama3-vasp/wandb/run-20240719_190916-xlq9i6oy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-4gpu
wandb: ⭐️ View project at https://wandb.ai/cx9/llama-dft
wandb: 🚀 View run at https://wandb.ai/cx9/llama-dft/runs/xlq9i6oy
  0%|          | 0/60 [00:00<?, ?it/s]  2%|▏         | 1/60 [01:26<1:24:47, 86.23s/it]                                                {'loss': 0.6085, 'grad_norm': 0.2941339910030365, 'learning_rate': 2e-05, 'epoch': 0.08}
  2%|▏         | 1/60 [01:26<1:24:47, 86.23s/it]  3%|▎         | 2/60 [02:15<1:02:29, 64.65s/it]                                                {'loss': 0.8097, 'grad_norm': 0.3567816913127899, 'learning_rate': 4e-05, 'epoch': 0.15}
  3%|▎         | 2/60 [02:15<1:02:29, 64.65s/it]  5%|▌         | 3/60 [02:43<45:21, 47.74s/it]                                                {'loss': 1.0471, 'grad_norm': 0.49573472142219543, 'learning_rate': 6e-05, 'epoch': 0.23}
  5%|▌         | 3/60 [02:43<45:21, 47.74s/it]  7%|▋         | 4/60 [04:04<56:46, 60.83s/it]                                              {'loss': 0.5724, 'grad_norm': 0.1851976066827774, 'learning_rate': 8e-05, 'epoch': 0.31}
  7%|▋         | 4/60 [04:04<56:46, 60.83s/it]  8%|▊         | 5/60 [04:49<50:26, 55.03s/it]                                              {'loss': 0.7226, 'grad_norm': 0.2546153962612152, 'learning_rate': 0.0001, 'epoch': 0.39}
  8%|▊         | 5/60 [04:49<50:26, 55.03s/it] 10%|█         | 6/60 [05:13<40:13, 44.69s/it]                                              {'loss': 0.9339, 'grad_norm': 0.4864761531352997, 'learning_rate': 0.00012, 'epoch': 0.46}
 10%|█         | 6/60 [05:13<40:13, 44.69s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.14s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                              
                                               [A{'eval_loss': 0.6026649475097656, 'eval_runtime': 41.3628, 'eval_samples_per_second': 3.312, 'eval_steps_per_second': 0.435, 'epoch': 0.46}
 10%|█         | 6/60 [05:55<40:13, 44.69s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A 12%|█▏        | 7/60 [07:16<1:01:55, 70.10s/it]                                                {'loss': 0.4918, 'grad_norm': 0.1321382373571396, 'learning_rate': 0.00014, 'epoch': 0.54}
 12%|█▏        | 7/60 [07:16<1:01:55, 70.10s/it] 13%|█▎        | 8/60 [08:00<53:37, 61.87s/it]                                                {'loss': 0.6304, 'grad_norm': 0.23398639261722565, 'learning_rate': 0.00016, 'epoch': 0.62}
 13%|█▎        | 8/60 [08:00<53:37, 61.87s/it] 15%|█▌        | 9/60 [08:26<43:01, 50.61s/it]                                              {'loss': 0.7537, 'grad_norm': 0.6097145676612854, 'learning_rate': 0.00018, 'epoch': 0.7}
 15%|█▌        | 9/60 [08:26<43:01, 50.61s/it] 17%|█▋        | 10/60 [09:50<50:56, 61.14s/it]                                               {'loss': 0.4674, 'grad_norm': 0.10387184470891953, 'learning_rate': 0.0002, 'epoch': 0.77}
 17%|█▋        | 10/60 [09:50<50:56, 61.14s/it] 18%|█▊        | 11/60 [10:38<46:31, 56.96s/it]                                               {'loss': 0.5453, 'grad_norm': 0.14708858728408813, 'learning_rate': 0.00019980267284282717, 'epoch': 0.85}
 18%|█▊        | 11/60 [10:38<46:31, 56.96s/it] 20%|██        | 12/60 [11:04<38:03, 47.56s/it]                                               {'loss': 0.6881, 'grad_norm': 0.2545546591281891, 'learning_rate': 0.0001992114701314478, 'epoch': 0.93}
 20%|██        | 12/60 [11:04<38:03, 47.56s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.58s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.04s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:16<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.14s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.19s/it][A                                               
                                               [A{'eval_loss': 0.4816088080406189, 'eval_runtime': 41.2576, 'eval_samples_per_second': 3.321, 'eval_steps_per_second': 0.436, 'epoch': 0.93}
 20%|██        | 12/60 [11:45<38:03, 47.56s/it]
100%|██████████| 18/18 [00:39<00:00,  2.19s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 22%|██▏       | 13/60 [12:53<51:55, 66.28s/it]                                               {'loss': 0.4564, 'grad_norm': 0.11523223668336868, 'learning_rate': 0.0001982287250728689, 'epoch': 1.01}
 22%|██▏       | 13/60 [12:53<51:55, 66.28s/it] 23%|██▎       | 14/60 [14:15<54:16, 70.80s/it]                                               {'loss': 0.4361, 'grad_norm': 0.12424896657466888, 'learning_rate': 0.0001968583161128631, 'epoch': 1.08}
 23%|██▎       | 14/60 [14:15<54:16, 70.80s/it] 25%|██▌       | 15/60 [14:59<47:14, 62.99s/it]                                               {'loss': 0.4714, 'grad_norm': 0.135407954454422, 'learning_rate': 0.00019510565162951537, 'epoch': 1.16}
 25%|██▌       | 15/60 [14:59<47:14, 62.99s/it] 27%|██▋       | 16/60 [15:30<39:07, 53.34s/it]                                               {'loss': 0.5189, 'grad_norm': 0.15486808121204376, 'learning_rate': 0.00019297764858882514, 'epoch': 1.24}
 27%|██▋       | 16/60 [15:30<39:07, 53.34s/it] 28%|██▊       | 17/60 [16:48<43:25, 60.59s/it]                                               {'loss': 0.421, 'grad_norm': 0.07203729450702667, 'learning_rate': 0.00019048270524660196, 'epoch': 1.32}
 28%|██▊       | 17/60 [16:48<43:25, 60.59s/it] 30%|███       | 18/60 [17:31<38:45, 55.36s/it]                                               {'loss': 0.4854, 'grad_norm': 0.10382799804210663, 'learning_rate': 0.00018763066800438636, 'epoch': 1.39}
 30%|███       | 18/60 [17:31<38:45, 55.36s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.59s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.04s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.4436091482639313, 'eval_runtime': 41.2923, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.436, 'epoch': 1.39}
 30%|███       | 18/60 [18:12<38:45, 55.36s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A 32%|███▏      | 19/60 [18:42<41:07, 60.18s/it]                                               {'loss': 0.5572, 'grad_norm': 0.23241065442562103, 'learning_rate': 0.00018443279255020152, 'epoch': 1.47}
 32%|███▏      | 19/60 [18:42<41:07, 60.18s/it] 33%|███▎      | 20/60 [20:05<44:40, 67.01s/it]                                               {'loss': 0.3723, 'grad_norm': 0.07882965356111526, 'learning_rate': 0.00018090169943749476, 'epoch': 1.55}
 33%|███▎      | 20/60 [20:05<44:40, 67.01s/it] 35%|███▌      | 21/60 [20:52<39:30, 60.79s/it]                                               {'loss': 0.494, 'grad_norm': 0.07847317308187485, 'learning_rate': 0.00017705132427757895, 'epoch': 1.63}
 35%|███▌      | 21/60 [20:52<39:30, 60.79s/it] 37%|███▋      | 22/60 [21:23<32:52, 51.92s/it]                                               {'loss': 0.5487, 'grad_norm': 0.12622793018817902, 'learning_rate': 0.00017289686274214118, 'epoch': 1.7}
 37%|███▋      | 22/60 [21:23<32:52, 51.92s/it] 38%|███▊      | 23/60 [22:44<37:24, 60.65s/it]                                               {'loss': 0.3982, 'grad_norm': 0.06090007722377777, 'learning_rate': 0.00016845471059286887, 'epoch': 1.78}
 38%|███▊      | 23/60 [22:44<37:24, 60.65s/it] 40%|████      | 24/60 [23:26<33:02, 55.07s/it]                                               {'loss': 0.4729, 'grad_norm': 0.07947969436645508, 'learning_rate': 0.000163742398974869, 'epoch': 1.86}
 40%|████      | 24/60 [23:26<33:02, 55.07s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.4232759177684784, 'eval_runtime': 41.3057, 'eval_samples_per_second': 3.317, 'eval_steps_per_second': 0.436, 'epoch': 1.86}
 40%|████      | 24/60 [24:07<33:02, 55.07s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A 42%|████▏     | 25/60 [24:37<34:50, 59.73s/it]                                               {'loss': 0.5534, 'grad_norm': 0.10237148404121399, 'learning_rate': 0.00015877852522924732, 'epoch': 1.94}
 42%|████▏     | 25/60 [24:37<34:50, 59.73s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 43%|████▎     | 26/60 [25:46<35:29, 62.63s/it]                                               {'loss': 0.3987, 'grad_norm': 0.05610772967338562, 'learning_rate': 0.00015358267949789966, 'epoch': 2.01}
 43%|████▎     | 26/60 [25:46<35:29, 62.63s/it] 45%|████▌     | 27/60 [27:07<37:32, 68.27s/it]                                               {'loss': 0.3722, 'grad_norm': 0.07959801703691483, 'learning_rate': 0.00014817536741017152, 'epoch': 2.09}
 45%|████▌     | 27/60 [27:07<37:32, 68.27s/it] 47%|████▋     | 28/60 [27:54<32:53, 61.67s/it]                                               {'loss': 0.4465, 'grad_norm': 0.07339897751808167, 'learning_rate': 0.00014257792915650728, 'epoch': 2.17}
 47%|████▋     | 28/60 [27:54<32:53, 61.67s/it] 48%|████▊     | 29/60 [28:31<28:04, 54.33s/it]                                               {'loss': 0.4721, 'grad_norm': 0.07397282123565674, 'learning_rate': 0.00013681245526846783, 'epoch': 2.25}
 48%|████▊     | 29/60 [28:31<28:04, 54.33s/it] 50%|█████     | 30/60 [29:47<30:22, 60.77s/it]                                               {'loss': 0.4096, 'grad_norm': 0.06280329823493958, 'learning_rate': 0.00013090169943749476, 'epoch': 2.32}
 50%|█████     | 30/60 [29:47<30:22, 60.77s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.19s/it][A                                               
                                               [A{'eval_loss': 0.4121684730052948, 'eval_runtime': 41.2934, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.436, 'epoch': 2.32}
 50%|█████     | 30/60 [30:28<30:22, 60.77s/it]
100%|██████████| 18/18 [00:39<00:00,  2.19s/it][A
                                               [A 52%|█████▏    | 31/60 [31:08<32:20, 66.92s/it]                                               {'loss': 0.4436, 'grad_norm': 0.07901133596897125, 'learning_rate': 0.0001248689887164855, 'epoch': 2.4}
 52%|█████▏    | 31/60 [31:08<32:20, 66.92s/it] 53%|█████▎    | 32/60 [31:43<26:46, 57.36s/it]                                               {'loss': 0.4892, 'grad_norm': 0.08667754381895065, 'learning_rate': 0.00011873813145857249, 'epoch': 2.48}
 53%|█████▎    | 32/60 [31:43<26:46, 57.36s/it] 55%|█████▌    | 33/60 [33:00<28:27, 63.25s/it]                                               {'loss': 0.3901, 'grad_norm': 0.061990637332201004, 'learning_rate': 0.00011253332335643043, 'epoch': 2.55}
 55%|█████▌    | 33/60 [33:00<28:27, 63.25s/it] 57%|█████▋    | 34/60 [33:43<24:45, 57.13s/it]                                               {'loss': 0.4142, 'grad_norm': 0.06965148448944092, 'learning_rate': 0.00010627905195293135, 'epoch': 2.63}
 57%|█████▋    | 34/60 [33:43<24:45, 57.13s/it] 58%|█████▊    | 35/60 [34:18<21:07, 50.69s/it]                                               {'loss': 0.4693, 'grad_norm': 0.07878931611776352, 'learning_rate': 0.0001, 'epoch': 2.71}
 58%|█████▊    | 35/60 [34:18<21:07, 50.69s/it] 60%|██████    | 36/60 [35:36<23:31, 58.80s/it]                                               {'loss': 0.3958, 'grad_norm': 0.08360651880502701, 'learning_rate': 9.372094804706867e-05, 'epoch': 2.79}
 60%|██████    | 36/60 [35:36<23:31, 58.80s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.59s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.40312090516090393, 'eval_runtime': 41.2801, 'eval_samples_per_second': 3.319, 'eval_steps_per_second': 0.436, 'epoch': 2.79}
 60%|██████    | 36/60 [36:17<23:31, 58.80s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A 62%|██████▏   | 37/60 [36:58<25:12, 65.75s/it]                                               {'loss': 0.4452, 'grad_norm': 0.07587368786334991, 'learning_rate': 8.746667664356956e-05, 'epoch': 2.86}
 62%|██████▏   | 37/60 [36:58<25:12, 65.75s/it] 63%|██████▎   | 38/60 [37:32<20:39, 56.33s/it]                                               {'loss': 0.4758, 'grad_norm': 0.08150235563516617, 'learning_rate': 8.126186854142752e-05, 'epoch': 2.94}
 63%|██████▎   | 38/60 [37:32<20:39, 56.33s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 65%|██████▌   | 39/60 [38:40<20:54, 59.76s/it]                                               {'loss': 0.3745, 'grad_norm': 0.06635363399982452, 'learning_rate': 7.513101128351454e-05, 'epoch': 3.02}
 65%|██████▌   | 39/60 [38:40<20:54, 59.76s/it] 67%|██████▋   | 40/60 [39:52<21:07, 63.36s/it]                                               {'loss': 0.3917, 'grad_norm': 0.06117238849401474, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.1}
 67%|██████▋   | 40/60 [39:52<21:07, 63.36s/it] 68%|██████▊   | 41/60 [40:30<17:42, 55.89s/it]                                               {'loss': 0.4227, 'grad_norm': 0.07152152061462402, 'learning_rate': 6.318754473153221e-05, 'epoch': 3.17}
 68%|██████▊   | 41/60 [40:30<17:42, 55.89s/it] 70%|███████   | 42/60 [41:10<15:19, 51.07s/it]                                               {'loss': 0.4444, 'grad_norm': 0.07825043797492981, 'learning_rate': 5.7422070843492734e-05, 'epoch': 3.25}
 70%|███████   | 42/60 [41:10<15:19, 51.07s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3962906301021576, 'eval_runtime': 41.2882, 'eval_samples_per_second': 3.318, 'eval_steps_per_second': 0.436, 'epoch': 3.25}
 70%|███████   | 42/60 [41:52<15:19, 51.07s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A 72%|███████▏  | 43/60 [43:10<20:16, 71.58s/it]                                               {'loss': 0.3729, 'grad_norm': 0.06861665844917297, 'learning_rate': 5.182463258982846e-05, 'epoch': 3.33}
 72%|███████▏  | 43/60 [43:10<20:16, 71.58s/it] 73%|███████▎  | 44/60 [43:51<16:38, 62.38s/it]                                               {'loss': 0.4095, 'grad_norm': 0.0657513290643692, 'learning_rate': 4.6417320502100316e-05, 'epoch': 3.41}
 73%|███████▎  | 44/60 [43:51<16:38, 62.38s/it] 75%|███████▌  | 45/60 [44:31<13:54, 55.63s/it]                                               {'loss': 0.4475, 'grad_norm': 0.07817399501800537, 'learning_rate': 4.12214747707527e-05, 'epoch': 3.48}
 75%|███████▌  | 45/60 [44:31<13:54, 55.63s/it] 77%|███████▋  | 46/60 [45:46<14:22, 61.59s/it]                                               {'loss': 0.3798, 'grad_norm': 0.06324272602796555, 'learning_rate': 3.6257601025131026e-05, 'epoch': 3.56}
 77%|███████▋  | 46/60 [45:46<14:22, 61.59s/it] 78%|███████▊  | 47/60 [46:28<12:04, 55.70s/it]                                               {'loss': 0.4258, 'grad_norm': 0.07631590217351913, 'learning_rate': 3.154528940713113e-05, 'epoch': 3.64}
 78%|███████▊  | 47/60 [46:28<12:04, 55.70s/it] 80%|████████  | 48/60 [47:09<10:16, 51.41s/it]                                               {'loss': 0.4148, 'grad_norm': 0.07588300853967667, 'learning_rate': 2.7103137257858868e-05, 'epoch': 3.72}
 80%|████████  | 48/60 [47:09<10:16, 51.41s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.59s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.04s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:16<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.19s/it][A                                               
                                               [A{'eval_loss': 0.3918208181858063, 'eval_runtime': 41.27, 'eval_samples_per_second': 3.32, 'eval_steps_per_second': 0.436, 'epoch': 3.72}
 80%|████████  | 48/60 [47:51<10:16, 51.41s/it]
100%|██████████| 18/18 [00:39<00:00,  2.19s/it][A
                                               [A 82%|████████▏ | 49/60 [49:07<13:04, 71.30s/it]                                               {'loss': 0.3631, 'grad_norm': 0.0632006824016571, 'learning_rate': 2.2948675722421086e-05, 'epoch': 3.79}
 82%|████████▏ | 49/60 [49:07<13:04, 71.30s/it] 83%|████████▎ | 50/60 [49:48<10:21, 62.10s/it]                                               {'loss': 0.4165, 'grad_norm': 0.08810894191265106, 'learning_rate': 1.9098300562505266e-05, 'epoch': 3.87}
 83%|████████▎ | 50/60 [49:48<10:21, 62.10s/it] 85%|████████▌ | 51/60 [50:26<08:15, 55.03s/it]                                               {'loss': 0.4838, 'grad_norm': 0.08337552100419998, 'learning_rate': 1.5567207449798515e-05, 'epoch': 3.95}
 85%|████████▌ | 51/60 [50:26<08:15, 55.03s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 87%|████████▋ | 52/60 [51:35<07:53, 59.25s/it]                                               {'loss': 0.3519, 'grad_norm': 0.06257524341344833, 'learning_rate': 1.2369331995613665e-05, 'epoch': 4.03}
 87%|████████▋ | 52/60 [51:35<07:53, 59.25s/it] 88%|████████▊ | 53/60 [52:45<07:16, 62.40s/it]                                               {'loss': 0.4133, 'grad_norm': 0.05862053483724594, 'learning_rate': 9.517294753398064e-06, 'epoch': 4.1}
 88%|████████▊ | 53/60 [52:45<07:16, 62.40s/it] 90%|█████████ | 54/60 [53:24<05:32, 55.48s/it]                                               {'loss': 0.4186, 'grad_norm': 0.06619567424058914, 'learning_rate': 7.022351411174866e-06, 'epoch': 4.18}
 90%|█████████ | 54/60 [53:24<05:32, 55.48s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.59s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.04s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.36s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.39006567001342773, 'eval_runtime': 41.402, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 0.435, 'epoch': 4.18}
 90%|█████████ | 54/60 [54:06<05:32, 55.48s/it]
100%|██████████| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A 92%|█████████▏| 55/60 [54:52<05:24, 64.95s/it]                                               {'loss': 0.413, 'grad_norm': 0.07688083499670029, 'learning_rate': 4.8943483704846475e-06, 'epoch': 4.26}
 92%|█████████▏| 55/60 [54:52<05:24, 64.95s/it] 93%|█████████▎| 56/60 [56:04<04:28, 67.19s/it]                                               {'loss': 0.3833, 'grad_norm': 0.0634462758898735, 'learning_rate': 3.1416838871368924e-06, 'epoch': 4.34}
 93%|█████████▎| 56/60 [56:04<04:28, 67.19s/it] 95%|█████████▌| 57/60 [56:43<02:56, 58.72s/it]                                               {'loss': 0.4019, 'grad_norm': 0.07438874244689941, 'learning_rate': 1.771274927131139e-06, 'epoch': 4.41}
 95%|█████████▌| 57/60 [56:43<02:56, 58.72s/it] 97%|█████████▋| 58/60 [57:28<01:49, 54.55s/it]                                               {'loss': 0.4296, 'grad_norm': 0.0689205676317215, 'learning_rate': 7.885298685522235e-07, 'epoch': 4.49}
 97%|█████████▋| 58/60 [57:28<01:49, 54.55s/it] 98%|█████████▊| 59/60 [58:39<00:59, 59.45s/it]                                               {'loss': 0.3735, 'grad_norm': 0.06020400673151016, 'learning_rate': 1.973271571728441e-07, 'epoch': 4.57}
 98%|█████████▊| 59/60 [58:39<00:59, 59.45s/it]100%|██████████| 60/60 [59:17<00:00, 53.19s/it]                                               {'loss': 0.401, 'grad_norm': 0.0623110868036747, 'learning_rate': 0.0, 'epoch': 4.65}
100%|██████████| 60/60 [59:17<00:00, 53.19s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|█         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|█▋        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|██▏       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|██▊       | 5/18 [00:09<00:27,  2.08s/it][A
 33%|███▎      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|███▉      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|████▍     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|█████     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|█████▌    | 10/18 [00:21<00:19,  2.44s/it][A
 61%|██████    | 11/18 [00:23<00:16,  2.31s/it][A
 67%|██████▋   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|███████▏  | 13/18 [00:27<00:10,  2.12s/it][A
 78%|███████▊  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|████████▎ | 15/18 [00:32<00:06,  2.20s/it][A
 89%|████████▉ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|█████████▍| 17/18 [00:36<00:02,  2.06s/it][A
100%|██████████| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3897935748100281, 'eval_runtime': 41.3099, 'eval_samples_per_second': 3.316, 'eval_steps_per_second': 0.436, 'epoch': 4.65}
100%|██████████| 60/60 [59:58<00:00, 53.19s/it]
100%|██████████| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
                                               {'train_runtime': 3622.5632, 'train_samples_per_second': 1.702, 'train_steps_per_second': 0.017, 'train_loss': 0.48354087869326273, 'epoch': 4.65}
100%|██████████| 60/60 [1:00:16<00:00, 53.19s/it]100%|██████████| 60/60 [1:00:16<00:00, 60.28s/it]
/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
wandb: - 0.008 MB of 0.008 MB uploadedwandb: \ 0.036 MB of 0.036 MB uploadedwandb: 
wandb: Run history:
wandb:               eval/loss █▄▃▂▂▁▁▁▁▁
wandb:            eval/runtime ▆▁▃▃▃▂▂▂█▄
wandb: eval/samples_per_second ▃█▆▆▆▇▆▇▁▅
wandb:   eval/steps_per_second ▁███████▁█
wandb:             train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:       train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████
wandb:         train/grad_norm ▇█▄▆▃▅▂▃▂▃▃▁▅▂▃▁▂▁▁▁▂▂▁▂▁▂▁▁▁▁▁▁▁▂▁▁▁▁▁▁
wandb:     train/learning_rate ▂▂▄▅▆▇██████▇▇▇▇▇▆▆▆▅▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁
wandb:              train/loss ▅█▄▇▃▅▃▄▃▂▄▂▄▁▄▂▄▂▂▃▂▃▂▃▂▃▂▂▁▂▁▂▁▂▁▂▂▁▂▂
wandb: 
wandb: Run summary:
wandb:                eval/loss 0.38979
wandb:             eval/runtime 41.3099
wandb:  eval/samples_per_second 3.316
wandb:    eval/steps_per_second 0.436
wandb:               total_flos 3.753688295989576e+17
wandb:              train/epoch 4.64516
wandb:        train/global_step 60
wandb:          train/grad_norm 0.06231
wandb:      train/learning_rate 0.0
wandb:               train/loss 0.401
wandb:               train_loss 0.48354
wandb:            train_runtime 3622.5632
wandb: train_samples_per_second 1.702
wandb:   train_steps_per_second 0.017
wandb: 
wandb: 🚀 View run finetune-4gpu at: https://wandb.ai/cx9/llama-dft/runs/xlq9i6oy
wandb: ⭐️ View project at: https://wandb.ai/cx9/llama-dft
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240719_190916-xlq9i6oy/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
