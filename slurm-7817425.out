current conda env is /global/scratch/users/chenxin0210/conda-env/llm
================
available nCPU is:
12
Thu Jul 18 22:09:45 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A40                     Off | 00000000:07:00.0 Off |                    0 |
|  0%   37C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA A40                     Off | 00000000:46:00.0 Off |                    0 |
|  0%   38C    P0              76W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   2  NVIDIA A40                     Off | 00000000:8A:00.0 Off |                    0 |
|  0%   34C    P0              71W / 300W |      4MiB / 46068MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   3  NVIDIA A40                     Off | 00000000:C5:00.0 Off |                    0 |
|  0%   37C    P0              71W / 300W |      4MiB / 46068MiB |      5%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
================
start running:
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_processes` was set to a value of `4`
		More than one GPU was found, enabling multi-GPU training.
		If this was unintended please pass in `--num_processes=1`.
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Currently logged in as: cx9. Use `wandb login --relogin` to force relogin
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
wandb: Appending key for api.wandb.ai to your netrc file: /global/home/users/chenxin0210/.netrc
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:01<03:04, 61.65s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:02<03:06, 62.13s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:02<03:06, 62.13s/it]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [01:02<03:06, 62.13s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:59<01:58, 59.20s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:59<01:58, 59.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:59<01:58, 59.38s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [01:59<01:58, 59.38s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:55<00:57, 57.97s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:56<00:58, 58.17s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:56<00:58, 58.18s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [02:56<00:58, 58.29s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 39.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 46.84s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 39.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 46.86s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 39.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 39.67s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 46.86s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [03:07<00:00, 46.86s/it]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /global/scratch/users/chenxin0210/llama3-vasp/wandb/run-20240718_221333-znbdborq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run finetune-4gpu
wandb: â­ï¸ View project at https://wandb.ai/cx9/llama-dft
wandb: ðŸš€ View run at https://wandb.ai/cx9/llama-dft/runs/znbdborq
  0%|          | 0/95 [00:00<?, ?it/s]  1%|          | 1/95 [01:02<1:37:15, 62.08s/it]                                                {'loss': 0.5299, 'grad_norm': 2.3173086643218994, 'learning_rate': 2e-05, 'epoch': 0.05}
  1%|          | 1/95 [01:02<1:37:15, 62.08s/it]  2%|â–         | 2/95 [01:48<1:22:23, 53.16s/it]                                                {'loss': 0.7806, 'grad_norm': 3.397233247756958, 'learning_rate': 4e-05, 'epoch': 0.1}
  2%|â–         | 2/95 [01:48<1:22:23, 53.16s/it]  3%|â–Ž         | 3/95 [02:20<1:06:05, 43.11s/it]                                                {'loss': 0.7388, 'grad_norm': 2.003695487976074, 'learning_rate': 6e-05, 'epoch': 0.15}
  3%|â–Ž         | 3/95 [02:20<1:06:05, 43.11s/it]  4%|â–         | 4/95 [02:42<53:03, 34.98s/it]                                                {'loss': 0.7444, 'grad_norm': 1.925537347793579, 'learning_rate': 8e-05, 'epoch': 0.21}
  4%|â–         | 4/95 [02:42<53:03, 34.98s/it]  5%|â–Œ         | 5/95 [03:09<47:51, 31.91s/it]                                              {'loss': 0.7849, 'grad_norm': 4.662044525146484, 'learning_rate': 0.0001, 'epoch': 0.26}
  5%|â–Œ         | 5/95 [03:09<47:51, 31.91s/it]  6%|â–‹         | 6/95 [04:04<59:24, 40.05s/it]                                              {'loss': 0.4801, 'grad_norm': 1.2170848846435547, 'learning_rate': 0.00012, 'epoch': 0.31}
  6%|â–‹         | 6/95 [04:04<59:24, 40.05s/it]  7%|â–‹         | 7/95 [04:40<56:35, 38.59s/it]                                              {'loss': 0.5317, 'grad_norm': 1.2081364393234253, 'learning_rate': 0.00014, 'epoch': 0.36}
  7%|â–‹         | 7/95 [04:40<56:35, 38.59s/it]  8%|â–Š         | 8/95 [05:07<50:24, 34.77s/it]                                              {'loss': 0.5296, 'grad_norm': 0.9276476502418518, 'learning_rate': 0.00016, 'epoch': 0.41}
  8%|â–Š         | 8/95 [05:07<50:24, 34.77s/it]  9%|â–‰         | 9/95 [05:25<42:40, 29.77s/it]                                              {'loss': 0.5426, 'grad_norm': 0.8570287227630615, 'learning_rate': 0.00018, 'epoch': 0.46}
  9%|â–‰         | 9/95 [05:25<42:40, 29.77s/it] 11%|â–ˆ         | 10/95 [06:01<44:51, 31.67s/it]                                               {'loss': 0.4918, 'grad_norm': 0.93939208984375, 'learning_rate': 0.0002, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:01<44:51, 31.67s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.47446292638778687, 'eval_runtime': 41.4045, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 0.435, 'epoch': 0.52}
 11%|â–ˆ         | 10/95 [06:43<44:51, 31.67s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 12%|â–ˆâ–        | 11/95 [07:44<1:14:33, 53.26s/it]                                                 {'loss': 0.4621, 'grad_norm': 0.6312372088432312, 'learning_rate': 0.0001999317060143023, 'epoch': 0.57}
 12%|â–ˆâ–        | 11/95 [07:44<1:14:33, 53.26s/it] 13%|â–ˆâ–Ž        | 12/95 [08:18<1:05:40, 47.48s/it]                                                 {'loss': 0.4866, 'grad_norm': 1.8438000679016113, 'learning_rate': 0.00019972691733857883, 'epoch': 0.62}
 13%|â–ˆâ–Ž        | 12/95 [08:18<1:05:40, 47.48s/it] 14%|â–ˆâ–Ž        | 13/95 [08:43<55:46, 40.82s/it]                                                 {'loss': 0.4961, 'grad_norm': 0.704510509967804, 'learning_rate': 0.0001993859136895274, 'epoch': 0.67}
 14%|â–ˆâ–Ž        | 13/95 [08:43<55:46, 40.82s/it] 15%|â–ˆâ–        | 14/95 [09:01<45:35, 33.78s/it]                                               {'loss': 0.4941, 'grad_norm': 0.7990629076957703, 'learning_rate': 0.0001989091608371146, 'epoch': 0.72}
 15%|â–ˆâ–        | 14/95 [09:01<45:35, 33.78s/it] 16%|â–ˆâ–Œ        | 15/95 [09:49<50:51, 38.15s/it]                                               {'loss': 0.4562, 'grad_norm': 0.4593840539455414, 'learning_rate': 0.0001982973099683902, 'epoch': 0.77}
 16%|â–ˆâ–Œ        | 15/95 [09:49<50:51, 38.15s/it] 17%|â–ˆâ–‹        | 16/95 [10:42<56:06, 42.61s/it]                                               {'loss': 0.4325, 'grad_norm': 0.3944952189922333, 'learning_rate': 0.00019755119679804367, 'epoch': 0.83}
 17%|â–ˆâ–‹        | 16/95 [10:42<56:06, 42.61s/it] 18%|â–ˆâ–Š        | 17/95 [11:15<51:26, 39.57s/it]                                               {'loss': 0.447, 'grad_norm': 0.40353068709373474, 'learning_rate': 0.00019667184042691875, 'epoch': 0.88}
 18%|â–ˆâ–Š        | 17/95 [11:15<51:26, 39.57s/it] 19%|â–ˆâ–‰        | 18/95 [11:37<44:20, 34.56s/it]                                               {'loss': 0.5012, 'grad_norm': 0.44284090399742126, 'learning_rate': 0.0001956604419500441, 'epoch': 0.93}
 19%|â–ˆâ–‰        | 18/95 [11:37<44:20, 34.56s/it] 20%|â–ˆâ–ˆ        | 19/95 [11:53<36:31, 28.83s/it]                                               {'loss': 0.55, 'grad_norm': 0.5315188765525818, 'learning_rate': 0.00019451838281608197, 'epoch': 0.98}
 20%|â–ˆâ–ˆ        | 19/95 [11:53<36:31, 28.83s/it] 21%|â–ˆâ–ˆ        | 20/95 [12:49<46:23, 37.12s/it]                                               {'loss': 0.3218, 'grad_norm': 0.3168112635612488, 'learning_rate': 0.00019324722294043558, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [12:49<46:23, 37.12s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.41457945108413696, 'eval_runtime': 41.4046, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 0.435, 'epoch': 1.03}
 21%|â–ˆâ–ˆ        | 20/95 [13:31<46:23, 37.12s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 22%|â–ˆâ–ˆâ–       | 21/95 [14:32<1:09:52, 56.66s/it]                                                 {'loss': 0.4191, 'grad_norm': 0.501531183719635, 'learning_rate': 0.00019184869857459232, 'epoch': 1.08}
 22%|â–ˆâ–ˆâ–       | 21/95 [14:32<1:09:52, 56.66s/it] 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:07<1:01:05, 50.21s/it]                                                 {'loss': 0.4165, 'grad_norm': 0.337245911359787, 'learning_rate': 0.0001903247199346129, 'epoch': 1.14}
 23%|â–ˆâ–ˆâ–Ž       | 22/95 [15:07<1:01:05, 50.21s/it] 24%|â–ˆâ–ˆâ–       | 23/95 [15:32<51:09, 42.63s/it]                                                 {'loss': 0.4568, 'grad_norm': 0.39539197087287903, 'learning_rate': 0.0001886773685920062, 'epoch': 1.19}
 24%|â–ˆâ–ˆâ–       | 23/95 [15:32<51:09, 42.63s/it] 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:48<41:13, 34.83s/it]                                               {'loss': 0.4572, 'grad_norm': 0.6213926076889038, 'learning_rate': 0.00018690889463055283, 'epoch': 1.24}
 25%|â–ˆâ–ˆâ–Œ       | 24/95 [15:48<41:13, 34.83s/it] 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:43<47:29, 40.71s/it]                                               {'loss': 0.3433, 'grad_norm': 0.31832966208457947, 'learning_rate': 0.00018502171357296144, 'epoch': 1.29}
 26%|â–ˆâ–ˆâ–‹       | 25/95 [16:43<47:29, 40.71s/it] 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:30<48:59, 42.61s/it]                                               {'loss': 0.4416, 'grad_norm': 0.38032442331314087, 'learning_rate': 0.00018301840308155507, 'epoch': 1.34}
 27%|â–ˆâ–ˆâ–‹       | 26/95 [17:30<48:59, 42.61s/it] 28%|â–ˆâ–ˆâ–Š       | 27/95 [18:00<44:06, 38.92s/it]                                               {'loss': 0.4303, 'grad_norm': 0.3556889593601227, 'learning_rate': 0.00018090169943749476, 'epoch': 1.39}
 28%|â–ˆâ–ˆâ–Š       | 27/95 [18:00<44:06, 38.92s/it] 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:23<37:57, 33.99s/it]                                               {'loss': 0.4269, 'grad_norm': 0.3334514796733856, 'learning_rate': 0.00017867449380334834, 'epoch': 1.45}
 29%|â–ˆâ–ˆâ–‰       | 28/95 [18:23<37:57, 33.99s/it] 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:43<32:58, 29.98s/it]                                               {'loss': 0.4949, 'grad_norm': 0.33692997694015503, 'learning_rate': 0.00017633982827411032, 'epoch': 1.5}
 31%|â–ˆâ–ˆâ–ˆ       | 29/95 [18:43<32:58, 29.98s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:43<42:02, 38.80s/it]                                               {'loss': 0.3223, 'grad_norm': 0.4216609001159668, 'learning_rate': 0.00017390089172206592, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [19:43<42:02, 38.80s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.20s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.38391146063804626, 'eval_runtime': 41.3946, 'eval_samples_per_second': 3.31, 'eval_steps_per_second': 0.435, 'epoch': 1.55}
 32%|â–ˆâ–ˆâ–ˆâ–      | 30/95 [20:24<42:02, 38.80s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:11<57:07, 53.55s/it]                                               {'loss': 0.3895, 'grad_norm': 0.29273146390914917, 'learning_rate': 0.00017136101544117525, 'epoch': 1.6}
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 31/95 [21:11<57:07, 53.55s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:39<48:17, 45.99s/it]                                               {'loss': 0.3867, 'grad_norm': 0.2366374433040619, 'learning_rate': 0.00016872366859692627, 'epoch': 1.65}
 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 32/95 [21:39<48:17, 45.99s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [21:59<39:36, 38.33s/it]                                               {'loss': 0.4086, 'grad_norm': 0.2465870976448059, 'learning_rate': 0.0001659924534878723, 'epoch': 1.7}
 35%|â–ˆâ–ˆâ–ˆâ–      | 33/95 [21:59<39:36, 38.33s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:30<36:44, 36.14s/it]                                               {'loss': 0.4318, 'grad_norm': 0.27615854144096375, 'learning_rate': 0.0001631711006253251, 'epoch': 1.75}
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 34/95 [22:30<36:44, 36.14s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:27<42:11, 42.19s/it]                                               {'loss': 0.3203, 'grad_norm': 0.2642683982849121, 'learning_rate': 0.00016026346363792567, 'epoch': 1.81}
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 35/95 [23:27<42:11, 42.19s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [24:03<39:51, 40.53s/it]                                               {'loss': 0.409, 'grad_norm': 0.29351624846458435, 'learning_rate': 0.00015727351400805052, 'epoch': 1.86}
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 36/95 [24:03<39:51, 40.53s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:29<34:57, 36.17s/it]                                               {'loss': 0.3903, 'grad_norm': 0.2875688970088959, 'learning_rate': 0.00015420533564724495, 'epoch': 1.91}
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 37/95 [24:29<34:57, 36.17s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:47<29:09, 30.69s/it]                                               {'loss': 0.4239, 'grad_norm': 0.27964791655540466, 'learning_rate': 0.0001510631193180907, 'epoch': 1.96}
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 38/95 [24:47<29:09, 30.69s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:25<30:32, 32.72s/it]                                               {'loss': 0.4264, 'grad_norm': 0.29524916410446167, 'learning_rate': 0.00014785115691012864, 'epoch': 2.01}
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 39/95 [25:25<30:32, 32.72s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:24<37:15, 40.64s/it]                                               {'loss': 0.2903, 'grad_norm': 0.2305556982755661, 'learning_rate': 0.00014457383557765386, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [26:24<37:15, 40.64s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3674316704273224, 'eval_runtime': 41.409, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.435, 'epoch': 2.06}
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 40/95 [27:05<37:15, 40.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:53<49:32, 55.05s/it]                                               {'loss': 0.3618, 'grad_norm': 0.2655516266822815, 'learning_rate': 0.00014123563174739037, 'epoch': 2.12}
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 41/95 [27:53<49:32, 55.05s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:19<41:06, 46.54s/it]                                               {'loss': 0.3882, 'grad_norm': 0.2958967089653015, 'learning_rate': 0.00013784110500423104, 'epoch': 2.17}
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 42/95 [28:19<41:06, 46.54s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:39<33:17, 38.42s/it]                                               {'loss': 0.3631, 'grad_norm': 0.27221980690956116, 'learning_rate': 0.00013439489186339282, 'epoch': 2.22}
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 43/95 [28:39<33:17, 38.42s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:15<32:09, 37.83s/it]                                               {'loss': 0.3591, 'grad_norm': 0.23850686848163605, 'learning_rate': 0.00013090169943749476, 'epoch': 2.27}
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 44/95 [29:15<32:09, 37.83s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [30:06<34:43, 41.67s/it]                                               {'loss': 0.3549, 'grad_norm': 0.21715688705444336, 'learning_rate': 0.0001273662990072083, 'epoch': 2.32}
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 45/95 [30:06<34:43, 41.67s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:38<31:49, 38.96s/it]                                               {'loss': 0.3684, 'grad_norm': 0.23163841664791107, 'learning_rate': 0.00012379351950426187, 'epoch': 2.37}
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 46/95 [30:38<31:49, 38.96s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [31:03<27:44, 34.67s/it]                                               {'loss': 0.3879, 'grad_norm': 0.21802100539207458, 'learning_rate': 0.00012018824091570103, 'epoch': 2.43}
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 47/95 [31:03<27:44, 34.67s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:20<23:02, 29.40s/it]                                               {'loss': 0.3879, 'grad_norm': 0.27217066287994385, 'learning_rate': 0.000116555387618413, 'epoch': 2.48}
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 48/95 [31:20<23:02, 29.40s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:08<26:48, 34.96s/it]                                               {'loss': 0.3499, 'grad_norm': 0.21777503192424774, 'learning_rate': 0.00011289992165302035, 'epoch': 2.53}
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 49/95 [32:08<26:48, 34.96s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:04<30:54, 41.21s/it]                                               {'loss': 0.3402, 'grad_norm': 0.21168628334999084, 'learning_rate': 0.00010922683594633021, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:04<30:54, 41.21s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3572523593902588, 'eval_runtime': 41.4038, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 0.435, 'epoch': 2.58}
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 50/95 [33:45<30:54, 41.21s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:29<39:48, 54.28s/it]                                               {'loss': 0.345, 'grad_norm': 0.23241044580936432, 'learning_rate': 0.000105541147491597, 'epoch': 2.63}
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 51/95 [34:29<39:48, 54.28s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:53<32:27, 45.30s/it]                                               {'loss': 0.3612, 'grad_norm': 0.22712789475917816, 'learning_rate': 0.00010184789049591299, 'epoch': 2.68}
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 52/95 [34:53<32:27, 45.30s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:08<25:25, 36.32s/it]                                               {'loss': 0.4477, 'grad_norm': 0.3151185214519501, 'learning_rate': 9.815210950408704e-05, 'epoch': 2.74}
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 53/95 [35:08<25:25, 36.32s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [36:07<29:27, 43.11s/it]                                               {'loss': 0.2595, 'grad_norm': 0.1713285744190216, 'learning_rate': 9.4458852508403e-05, 'epoch': 2.79}
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 54/95 [36:07<29:27, 43.11s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:47<28:02, 42.05s/it]                                               {'loss': 0.3578, 'grad_norm': 0.2705511450767517, 'learning_rate': 9.077316405366981e-05, 'epoch': 2.84}
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 55/95 [36:47<28:02, 42.05s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:16<24:44, 38.07s/it]                                               {'loss': 0.3362, 'grad_norm': 0.19712871313095093, 'learning_rate': 8.710007834697969e-05, 'epoch': 2.89}
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 56/95 [37:16<24:44, 38.07s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:37<20:56, 33.07s/it]                                               {'loss': 0.3788, 'grad_norm': 0.243755504488945, 'learning_rate': 8.344461238158699e-05, 'epoch': 2.94}
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 57/95 [37:37<20:56, 33.07s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [38:00<18:30, 30.01s/it]                                               {'loss': 0.4376, 'grad_norm': 0.26799076795578003, 'learning_rate': 7.9811759084299e-05, 'epoch': 2.99}
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 58/95 [38:00<18:30, 30.01s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [39:01<23:31, 39.20s/it]                                               {'loss': 0.2643, 'grad_norm': 0.164021298289299, 'learning_rate': 7.620648049573815e-05, 'epoch': 3.05}
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 59/95 [39:01<23:31, 39.20s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:44<23:30, 40.30s/it]                                               {'loss': 0.3416, 'grad_norm': 0.2390194833278656, 'learning_rate': 7.263370099279172e-05, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [39:44<23:30, 40.30s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:23,  1.60s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.22s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.38s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.20s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.34806251525878906, 'eval_runtime': 41.3743, 'eval_samples_per_second': 3.311, 'eval_steps_per_second': 0.435, 'epoch': 3.1}
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 60/95 [40:25<23:30, 40.30s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:39<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [41:02<29:18, 51.73s/it]                                               {'loss': 0.3226, 'grad_norm': 0.22547881305217743, 'learning_rate': 6.909830056250527e-05, 'epoch': 3.15}
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 61/95 [41:02<29:18, 51.73s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:24<23:37, 42.94s/it]                                               {'loss': 0.3459, 'grad_norm': 0.2365858107805252, 'learning_rate': 6.560510813660719e-05, 'epoch': 3.2}
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 62/95 [41:24<23:37, 42.94s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:45<19:19, 36.24s/it]                                               {'loss': 0.3603, 'grad_norm': 0.26133570075035095, 'learning_rate': 6.215889499576898e-05, 'epoch': 3.25}
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 63/95 [41:45<19:19, 36.24s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:46<22:29, 43.54s/it]                                               {'loss': 0.2372, 'grad_norm': 0.19997431337833405, 'learning_rate': 5.876436825260967e-05, 'epoch': 3.3}
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 64/95 [42:46<22:29, 43.54s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:27<21:31, 43.04s/it]                                               {'loss': 0.3589, 'grad_norm': 0.27659842371940613, 'learning_rate': 5.542616442234618e-05, 'epoch': 3.35}
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 65/95 [43:27<21:31, 43.04s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:55<18:35, 38.47s/it]                                               {'loss': 0.3326, 'grad_norm': 0.22111129760742188, 'learning_rate': 5.214884308987136e-05, 'epoch': 3.41}
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 66/95 [43:55<18:35, 38.47s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:16<15:28, 33.16s/it]                                               {'loss': 0.3626, 'grad_norm': 0.20732896029949188, 'learning_rate': 4.893688068190932e-05, 'epoch': 3.46}
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 67/95 [44:16<15:28, 33.16s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:47<14:38, 32.53s/it]                                               {'loss': 0.3701, 'grad_norm': 0.2752240002155304, 'learning_rate': 4.5794664352755055e-05, 'epoch': 3.51}
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 68/95 [44:47<14:38, 32.53s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:43<17:06, 39.49s/it]                                               {'loss': 0.2828, 'grad_norm': 0.15510359406471252, 'learning_rate': 4.272648599194948e-05, 'epoch': 3.56}
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 69/95 [45:43<17:06, 39.49s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:20<16:09, 38.77s/it]                                               {'loss': 0.3102, 'grad_norm': 0.21074283123016357, 'learning_rate': 3.973653636207437e-05, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [46:20<16:09, 38.77s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.05s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.34s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3427753746509552, 'eval_runtime': 41.3991, 'eval_samples_per_second': 3.309, 'eval_steps_per_second': 0.435, 'epoch': 3.61}
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 70/95 [47:01<16:09, 38.77s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:34<19:47, 49.46s/it]                                               {'loss': 0.3394, 'grad_norm': 0.21040113270282745, 'learning_rate': 3.682889937467493e-05, 'epoch': 3.66}
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 71/95 [47:34<19:47, 49.46s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:52<15:20, 40.04s/it]                                               {'loss': 0.3278, 'grad_norm': 0.2687945067882538, 'learning_rate': 3.400754651212776e-05, 'epoch': 3.72}
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 72/95 [47:52<15:20, 40.04s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:34<14:54, 40.64s/it]                                               {'loss': 0.2868, 'grad_norm': 0.3946947455406189, 'learning_rate': 3.1276331403073735e-05, 'epoch': 3.77}
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 73/95 [48:34<14:54, 40.64s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:29<15:41, 44.84s/it]                                               {'loss': 0.3003, 'grad_norm': 0.17307142913341522, 'learning_rate': 2.8638984558824777e-05, 'epoch': 3.82}
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 74/95 [49:29<15:41, 44.84s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [50:04<14:00, 42.03s/it]                                               {'loss': 0.3337, 'grad_norm': 0.19986805319786072, 'learning_rate': 2.6099108277934103e-05, 'epoch': 3.87}
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 75/95 [50:04<14:00, 42.03s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:29<11:41, 36.90s/it]                                               {'loss': 0.3471, 'grad_norm': 0.20502254366874695, 'learning_rate': 2.36601717258897e-05, 'epoch': 3.92}
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 76/95 [50:29<11:41, 36.90s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:46<09:13, 30.76s/it]                                               {'loss': 0.4055, 'grad_norm': 0.26000088453292847, 'learning_rate': 2.132550619665168e-05, 'epoch': 3.97}
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 77/95 [50:46<09:13, 30.76s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:36<10:24, 36.71s/it]                                               {'loss': 0.269, 'grad_norm': 0.20912940800189972, 'learning_rate': 1.9098300562505266e-05, 'epoch': 4.03}
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 78/95 [51:36<10:24, 36.71s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:31<11:15, 42.20s/it]                                               {'loss': 0.258, 'grad_norm': 0.13412976264953613, 'learning_rate': 1.6981596918444953e-05, 'epoch': 4.08}
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 79/95 [52:31<11:15, 42.20s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [53:05<09:55, 39.73s/it]                                               {'loss': 0.3232, 'grad_norm': 0.17360225319862366, 'learning_rate': 1.4978286427038601e-05, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [53:05<09:55, 39.73s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.99s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.07s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.3403901159763336, 'eval_runtime': 41.4202, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.435, 'epoch': 4.13}
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 80/95 [53:47<09:55, 39.73s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:18<11:35, 49.70s/it]                                               {'loss': 0.3335, 'grad_norm': 0.17369034886360168, 'learning_rate': 1.3091105369447165e-05, 'epoch': 4.18}
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 81/95 [54:18<11:35, 49.70s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:36<08:41, 40.10s/it]                                               {'loss': 0.2873, 'grad_norm': 0.2206384390592575, 'learning_rate': 1.1322631407993811e-05, 'epoch': 4.23}
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 82/95 [54:36<08:41, 40.10s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:24<08:31, 42.59s/it]                                               {'loss': 0.3012, 'grad_norm': 0.2117202877998352, 'learning_rate': 9.675280065387116e-06, 'epoch': 4.28}
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 83/95 [55:24<08:31, 42.59s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:15<08:16, 45.11s/it]                                               {'loss': 0.3193, 'grad_norm': 0.156878262758255, 'learning_rate': 8.151301425407699e-06, 'epoch': 4.34}
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 84/95 [56:15<08:16, 45.11s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:48<06:53, 41.32s/it]                                               {'loss': 0.3116, 'grad_norm': 0.1724098175764084, 'learning_rate': 6.75277705956443e-06, 'epoch': 4.39}
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 85/95 [56:48<06:53, 41.32s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [57:11<05:22, 35.86s/it]                                               {'loss': 0.3286, 'grad_norm': 0.1942257136106491, 'learning_rate': 5.481617183918053e-06, 'epoch': 4.44}
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 86/95 [57:11<05:22, 35.86s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:26<03:55, 29.45s/it]                                               {'loss': 0.3851, 'grad_norm': 0.3690452575683594, 'learning_rate': 4.339558049955927e-06, 'epoch': 4.49}
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 87/95 [57:26<03:55, 29.45s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:25<04:28, 38.39s/it]                                               {'loss': 0.2534, 'grad_norm': 0.1505725383758545, 'learning_rate': 3.3281595730812575e-06, 'epoch': 4.54}
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 88/95 [58:25<04:28, 38.39s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [59:07<03:56, 39.43s/it]                                               {'loss': 0.322, 'grad_norm': 0.1764608472585678, 'learning_rate': 2.4488032019563402e-06, 'epoch': 4.59}
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 89/95 [59:07<03:56, 39.43s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:37<03:03, 36.64s/it]                                               {'loss': 0.3109, 'grad_norm': 0.18319250643253326, 'learning_rate': 1.7026900316098215e-06, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [59:37<03:03, 36.64s/it]
  0%|          | 0/18 [00:00<?, ?it/s][A
 11%|â–ˆ         | 2/18 [00:02<00:18,  1.13s/it][A
 17%|â–ˆâ–‹        | 3/18 [00:04<00:24,  1.61s/it][A
 22%|â–ˆâ–ˆâ–       | 4/18 [00:07<00:28,  2.06s/it][A
 28%|â–ˆâ–ˆâ–Š       | 5/18 [00:09<00:27,  2.09s/it][A
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 6/18 [00:11<00:26,  2.23s/it][A
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 7/18 [00:14<00:25,  2.35s/it][A
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/18 [00:17<00:23,  2.39s/it][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 9/18 [00:18<00:19,  2.15s/it][A
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 10/18 [00:21<00:19,  2.45s/it][A
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 11/18 [00:23<00:16,  2.32s/it][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 12/18 [00:26<00:14,  2.37s/it][A
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 13/18 [00:27<00:10,  2.13s/it][A
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 14/18 [00:29<00:07,  1.98s/it][A
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 15/18 [00:32<00:06,  2.21s/it][A
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 16/18 [00:34<00:04,  2.26s/it][A
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 17/18 [00:36<00:02,  2.06s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:38<00:00,  2.20s/it][A                                               
                                               [A{'eval_loss': 0.342583030462265, 'eval_runtime': 41.411, 'eval_samples_per_second': 3.308, 'eval_steps_per_second': 0.435, 'epoch': 4.65}
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 90/95 [1:00:18<03:03, 36.64s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 18/18 [00:40<00:00,  2.20s/it][A
                                               [A/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:48<03:07, 46.97s/it]                                                 {'loss': 0.3197, 'grad_norm': 0.20667459070682526, 'learning_rate': 1.0908391628854041e-06, 'epoch': 4.7}
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 91/95 [1:00:48<03:07, 46.97s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:01:13<02:01, 40.54s/it]                                                 {'loss': 0.3423, 'grad_norm': 0.35639941692352295, 'learning_rate': 6.140863104726391e-07, 'epoch': 4.75}
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 92/95 [1:01:13<02:01, 40.54s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:02:13<01:32, 46.19s/it]                                                 {'loss': 0.2599, 'grad_norm': 0.1456100344657898, 'learning_rate': 2.7308266142119785e-07, 'epoch': 4.8}
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 93/95 [1:02:13<01:32, 46.19s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:53<00:44, 44.44s/it]                                                 {'loss': 0.3224, 'grad_norm': 0.18779584765434265, 'learning_rate': 6.829398569770939e-08, 'epoch': 4.85}
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 94/95 [1:02:53<00:44, 44.44s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:20<00:00, 39.12s/it]                                                 {'loss': 0.2966, 'grad_norm': 0.22051893174648285, 'learning_rate': 0.0, 'epoch': 4.9}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [1:03:20<00:00, 39.12s/it]/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank2]: Traceback (most recent call last):
[rank2]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank2]:     trainer.train()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank2]:     output = super().train(*args, **kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank2]:     return inner_training_loop(
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank2]:     self._load_best_model()
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank2]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank2]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank2]:     adapters_weights = safe_load_file(filename, device=device)
[rank2]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank2]:     result[k] = f.get_tensor(k)
[rank2]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank1]: Traceback (most recent call last):
[rank1]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank1]:     trainer.train()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank1]:     output = super().train(*args, **kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank1]:     self._load_best_model()
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank1]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank1]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank1]:     adapters_weights = safe_load_file(filename, device=device)
[rank1]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank1]:     result[k] = f.get_tensor(k)
[rank1]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
    trainer.train()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
    output = super().train(*args, **kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
    return inner_training_loop(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
    self._load_best_model()
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
    model.load_adapter(self.state.best_model_checkpoint, active_adapter)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
    adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
    adapters_weights = safe_load_file(filename, device=device)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
    result[k] = f.get_tensor(k)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
[rank3]: Traceback (most recent call last):
[rank3]:   File "/global/scratch/users/chenxin0210/llama3-vasp/run-llama.py", line 139, in <module>
[rank3]:     trainer.train()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/trl/trainer/sft_trainer.py", line 451, in train
[rank3]:     output = super().train(*args, **kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1932, in train
[rank3]:     return inner_training_loop(
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2393, in _inner_training_loop
[rank3]:     self._load_best_model()
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2690, in _load_best_model
[rank3]:     model.load_adapter(self.state.best_model_checkpoint, active_adapter)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/peft_model.py", line 984, in load_adapter
[rank3]:     adapters_weights = load_peft_weights(model_id, device=torch_device, **hf_hub_download_kwargs)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 444, in load_peft_weights
[rank3]:     adapters_weights = safe_load_file(filename, device=device)
[rank3]:   File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/safetensors/torch.py", line 310, in load_file
[rank3]:     result[k] = f.get_tensor(k)
[rank3]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 
W0718 23:17:07.840000 22822637220352 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1078168 closing signal SIGTERM
W0718 23:17:07.840000 22822637220352 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1078169 closing signal SIGTERM
W0718 23:17:07.841000 22822637220352 torch/distributed/elastic/multiprocessing/api.py:851] Sending process 1078171 closing signal SIGTERM
E0718 23:17:08.906000 22822637220352 torch/distributed/elastic/multiprocessing/api.py:826] failed (exitcode: 1) local_rank: 2 (pid: 1078170) of binary: /global/scratch/users/chenxin0210/conda-env/llm/bin/python
Traceback (most recent call last):
  File "/global/scratch/users/chenxin0210/conda-env/llm/bin/accelerate", line 8, in <module>
    sys.exit(main())
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/accelerate_cli.py", line 48, in main
    args.func(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 1088, in launch_command
    multi_gpu_launcher(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/accelerate/commands/launch.py", line 733, in multi_gpu_launcher
    distrib_run.run(args)
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/run.py", line 870, in run
    elastic_launch(
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/global/scratch/users/chenxin0210/conda-env/llm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 263, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
run-llama.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-18_23:17:07
  host      : n0000.es1
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 1078170)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
